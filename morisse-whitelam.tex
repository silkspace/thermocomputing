\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,bm}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{margin=1in}

\title{
\vspace{-1cm}
\bfseries
The Criticality Engine:\\
Online Learning in Thermodynamic Neural Computers
}

\author{
Alexander Morisse$^{1,*}$ \qquad
Stephen Whitelam$^{2,\dagger}$ \qquad
Isaac Tamblyn$^{3,\ddagger}$
\\[0.5em]
\small{$^1$Independent Researcher}\\
\small{$^2$Molecular Foundry, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA}\\
\small{$^3$Department of Physics, University of Ottawa, Ottawa, ON, Canada}\\[0.3em]
\small{$^*$alex@morisse.ai \quad $^\dagger$swhitelam@lbl.gov \quad $^\ddagger$isaac.tamblyn@uottawa.ca}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We introduce the \emph{Criticality Engine}, a computational architecture that performs both inference and online learning by exploiting the intrinsic stochastic dynamics of thermodynamic systems.
The key theoretical insight is that learning in such systems is \emph{constraint satisfaction}, not optimization: the Onsager-Machlup action $S[\mathbf{x}(t)] = \int (\dot{\mathbf{x}} + \nabla V)^2 dt / 4\mu k_{\rm B}T$ measures trajectory likelihood, and as noise decreases, probability concentrates on least-action paths while competing trajectories are exponentially suppressed.
Temperature acts as a Lagrangian multiplier controlling constraint tightness, and forward-backward trajectory symmetry (detailed balance) provides additional constraints that accelerate convergence.
This perspective explains why trajectory-based learning is fast: we demonstrate $>$99\% parameter recovery in 10--20 epochs on synthetic tasks, and 76\% MNIST classification accuracy on a holdout test set after a single epoch---compared to thousands of epochs typical in conventional neural network training.
CMOS p-bits biased at threshold naturally implement these dynamics in hardware, and the learning rules decompose into local operations suitable for on-chip gradient estimation.
We present a hardware blueprint with measurement plane, local statistics layer, and dual-bank reconfiguration fabric, establishing a route toward trainable thermodynamic neural computers with potential for significant energy reduction.
\end{abstract}

\section{Introduction}

Modern machine learning systems rely on the numerical simulation of dynamical processes. 
Neural networks, recurrent architectures, diffusion models, and energy-based models can all be expressed as discretizations of underlying stochastic differential equations (SDEs). 
Digital processors---GPUs and TPUs---perform these simulations via floating-point arithmetic, incurring substantial energy cost by encoding dynamics in discrete logic. 
An alternative paradigm is to realize the dynamics directly in hardware: a \emph{thermodynamic computer}.

Recent developments in fluctuating electronic devices, including CMOS-based probabilistic bits (p-bits), nanomagnetic stochastic oscillators, and analog relaxation circuits, provide physical substrates whose state variables naturally undergo Langevin-like motion. 
Existing thermodynamic hardware accelerators (e.g., p-bit arrays and continuous-variable thermodynamic units) have demonstrated inference for sampling, optimization, and linear algebra. 
However, no current architecture supports \emph{training} on-chip---the ability to estimate gradients of task-specific objectives and modify physical couplings accordingly.

In this work we propose the \emph{Criticality Engine}, an architecture designed to close this gap.
The key insight is twofold.
First, CMOS p-bits biased at threshold are inherently critical: the probability $p(s_i=1) = \sigma(h_i)$ realizes Gibbs sampling in physics, not simulation.
Second, and more fundamentally, learning in such systems is \emph{constraint satisfaction}, not optimization.
The Onsager-Machlup action functional measures how well model parameters explain observed trajectories; correct parameters minimize the action, making observed dynamics maximally likely relative to competing paths.
Temperature acts as a Lagrangian multiplier, and forward-backward symmetry (detailed balance) provides additional constraints.
This perspective explains why trajectory-based learning converges rapidly---in 10--20 epochs rather than thousands---because projecting onto a constraint surface is geometrically simpler than searching a high-dimensional loss landscape.

\textbf{Contributions.}
(i) A finite-time trajectory likelihood objective for thermodynamic arrays that yields closed-form gradients from local statistics.
(ii) Gradient estimators for biases $b_i$ and couplings $J_{ij}$ requiring only local state increments and neighbor values---suitable for on-chip implementation.
(iii) A full-chip blueprint with measurement plane, local statistics layer, gradient engine, and dual-bank reconfiguration fabric.
(iv) Initial empirical validation on synthetic parameter recovery and MNIST reconstruction, demonstrating rapid convergence.

Section~\ref{sec:model} introduces the physical model; Section~\ref{sec:architecture} describes the processor architecture; Section~\ref{sec:learning_rules} derives the local gradient estimators; Section~\ref{sec:constraint} interprets learning as constraint satisfaction via the Onsager-Machlup action; Section~\ref{sec:hardware} maps the architecture to CMOS.

\section{Physical Model}
\label{sec:model}

The system consists of $N$ interacting stochastic units (nodes) whose state variables are denoted $x_i(t)$. 
Depending on implementation, $x_i$ may represent:
\begin{itemize}
    \item a fluctuating voltage or current (continuous variable),
    \item the metastable output of a CMOS p-bit (binary variable),
    \item the magnetization state of a superparamagnetic nanomagnet.
\end{itemize}

We model the dynamics by the overdamped Langevin equation
\begin{equation}
\frac{dx_i}{dt} = f_i(\mathbf{x};\bm{\theta}) + \sqrt{2D_i}\,\eta_i(t),
\label{eq:langevin}
\end{equation}
where $\bm{\theta}$ denotes tunable physical parameters (biases, couplings), $D_i$ encodes noise strength, and $\eta_i(t)$ are independent white-noise processes.

In many realizations the deterministic drift derives from an energy function:
\begin{equation}
f_i(\mathbf{x};\bm{\theta}) = -\frac{\partial U(\mathbf{x};\bm{\theta})}{\partial x_i},
\end{equation}
so that (\ref{eq:langevin}) defines a relaxation toward a thermodynamic equilibrium distribution
\begin{equation}
p_{\bm{\theta}}(\mathbf{x}) \propto \exp\!\left[-\beta U(\mathbf{x};\bm{\theta})\right].
\end{equation}

We emphasize the analogy with recurrent neural networks (RNNs):
\begin{align*}
f(\mathbf{x}) &\;\longleftrightarrow\; \text{hidden-state update in a continuous-time RNN}, \\
U(\mathbf{x}) &\;\longleftrightarrow\; \text{energy function of an energy-based model}.
\end{align*}
Neural networks are programmable oscillators; training shapes their attractor geometry.
Our architecture treats these structures as \emph{physical}, not virtual.

\textbf{Modeling regimes.}
The dynamics (\ref{eq:langevin}) are written in continuous state space $x_i \in \mathbb{R}$.
When the potential $U$ has multiple wells (e.g., the $\varphi^4$ form with $J_2 < 0$), $x_i$ fluctuates around discrete attractors, and the system can be coarse-grained to an effective binary model with $s_i \in \{-1,+1\}$.
CMOS p-bits operate in this regime: the cross-coupled inverters create a double-well potential, and thermal fluctuations drive transitions.
The learning framework applies in both limits---continuous Langevin for analog circuits, effective Glauber dynamics for digital p-bits---with the same local gradient estimators.
We do not require the drift $f$ to derive from a potential; the gradient case is a special instance where detailed balance holds.

\section{Processor Architecture}
\label{sec:architecture}

The Criticality Engine realizes learning through a closed loop: physical dynamics generate trajectories, measurements extract statistics, and parameter updates reshape the energy landscape.
Figure~\ref{fig:architecture} shows the five integrated subsystems and their data flow.
We describe each in turn, following a single learning cycle.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{paper_figures/architecture_schematic.png}
\caption{Processor architecture of the Criticality Engine. The Thermodynamic Array (blue) implements Langevin dynamics; the Measurement Plane (orange) samples trajectories; the Local Statistics Layer (purple) computes correlations; the Gradient Engine (green) derives parameter updates; and the Reconfiguration Fabric (red) applies new parameters atomically. Arrows indicate data flow through one learning cycle.}
\label{fig:architecture}
\end{figure}

\subsection{Thermodynamic Array (T-array)}

At the heart of the processor lies a physical substrate that \emph{is} the neural network---not a simulation of one.
The T-array consists of $N$ stochastic nodes whose state variables $x_i(t)$ evolve according to Eq.~(\ref{eq:langevin}).
Each node experiences:
\begin{itemize}
\item A \textbf{local bias} $b_i$ realized as a programmable current source,
\item \textbf{Pairwise couplings} $J_{ij}$ implemented via current mirrors that inject current proportional to neighboring states,
\item \textbf{Intrinsic noise} from thermal fluctuations at the operating temperature $T$.
\end{itemize}

The key insight is that CMOS transistors biased at threshold are inherently bistable (the $\varphi^4$ potential), and their thermal fluctuations provide the stochastic drive $\eta_i(t)$ for free.
No random number generator is needed; the physics itself samples from the Gibbs distribution.

For ensemble averaging, the T-array may be replicated $R$ times, with all replicas sharing the same parameters but evolving independently.
This provides $R$ parallel trajectory samples per learning step.

\subsection{Measurement Plane}

Learning requires observing trajectories, not just equilibrium samples.
The Measurement Plane is a high-impedance sampling network that captures snapshots of all node states at programmable times $\{t_0, t_1, \ldots, t_K\}$.

Each observation consists of:
\begin{equation}
\{x_i^{(r)}(t_k)\} \quad \text{for } i = 1,\ldots,N; \quad r = 1,\ldots,R; \quad k = 0,\ldots,K.
\end{equation}
Sample-and-hold circuits latch analog voltages into local registers with minimal perturbation to the ongoing dynamics.
The measurement bandwidth must exceed the correlation time $\tau_{\rm corr} \sim \lambda_{\min}^{-1}$ to capture independent samples.

Critically, the Measurement Plane observes \emph{displacements} $\Delta x_i^k = x_i(t_{k+1}) - x_i(t_k)$---the raw material for trajectory-based learning.

\subsection{Local Statistics Layer}

The gradient formulas of Section~\ref{sec:learning_rules} require only local quantities: displacements, forces, and pairwise correlations.
The Local Statistics Layer computes these on-chip using dedicated correlator blocks:
\begin{equation}
m_i(t_k) = \frac{1}{R}\sum_{r=1}^R x_i^{(r)}(t_k), \qquad
C_{ij}(t_k) = \frac{1}{R}\sum_{r=1}^R x_i^{(r)}(t_k) x_j^{(r)}(t_k).
\end{equation}

These are \emph{sufficient statistics} for gradient estimation: the mean $m_i$ enters the bias gradient, and the correlation $C_{ij}$ enters the coupling gradient.
Because each correlator operates on a local neighborhood, the computation is embarrassingly parallel and scales linearly with node count.

\textbf{What is stored locally.}
Each node $i$ maintains only:
(1) its current state $x_i(t_k)$,
(2) the displacement $\Delta x_i^k = x_i(t_{k+1}) - x_i(t_k)$, and
(3) states of its neighbors $\{x_j : j \in \mathcal{N}(i)\}$.
The gradient for bias $b_i$ is a function of $(x_i, \Delta x_i)$ alone.
The gradient for coupling $J_{ij}$ is a product of quantities at nodes $i$ and $j$---no global communication required.
This locality is the key to scalability: a 1000-node array computes 1000 bias gradients and $\sim$10,000 coupling gradients (for degree-10 connectivity) entirely in parallel.

\subsection{Gradient Engine}

A lightweight digital core---as simple as a vector microcontroller---reads the statistics and computes parameter updates according to the learning rules:
\begin{align}
\Delta J_{ij} &\propto \text{(observed velocity)} - \text{(predicted velocity)}, \\
\Delta b_i &\propto \text{(velocity mismatch at node } i\text{)}.
\end{align}
The precise formulas are derived in Section~\ref{sec:learning_rules}; the key point is that they involve only quantities already computed by the Local Statistics Layer.

The Gradient Engine writes updated parameters to a \emph{shadow bank}---a separate memory that does not yet affect the T-array.
This separation is essential: it allows the T-array to continue evolving under stable parameters while updates are being computed.

\subsection{Reconfiguration Fabric}

The final stage closes the learning loop.
A dual-bank DAC (digital-to-analog converter) array holds two complete parameter sets: the \emph{active bank} currently driving the T-array, and the \emph{shadow bank} receiving updates from the Gradient Engine.

When an update cycle completes, a global bank-select signal atomically swaps the roles of the two banks.
This provides:
\begin{itemize}
\item \textbf{Glitch-free updates:} The T-array never sees partially-written parameters.
\item \textbf{Instant rollback:} If an update causes instability, the previous parameters remain in the shadow bank.
\item \textbf{Class-conditional switching:} For multi-class problems, separate bias banks $\{\mathbf{b}_0, \ldots, \mathbf{b}_{K-1}\}$ can be swapped to select different attractors (Section~\ref{sec:experiments}).
\end{itemize}

DAC settling times ($\sim$10--100~ns) set the minimum interval between parameter updates, enabling update rates of 10--100~MHz---fast enough for online learning.

\subsection{The Learning Cycle}

A complete learning cycle proceeds as follows:
\begin{enumerate}
\item \textbf{Initialize:} Load data sample $\mathbf{x}_{\rm data}$ into the T-array (optionally with added noise).
\item \textbf{Evolve:} Let the system relax under Langevin dynamics for time $T$.
\item \textbf{Measure:} The Measurement Plane captures the trajectory $\{\mathbf{x}(t_k)\}$.
\item \textbf{Compute:} Local Statistics Layer aggregates correlations; Gradient Engine computes $\Delta\bm{\theta}$.
\item \textbf{Update:} New parameters written to shadow bank; bank-select swaps active/shadow.
\item \textbf{Repeat:} Next data sample begins the cycle anew.
\end{enumerate}

The entire cycle can complete in microseconds, limited primarily by the relaxation time of the T-array.
With MHz-scale p-bit dynamics, this enables thousands of parameter updates per second---true online learning in hardware.

\textbf{What is computed where.}
\begin{itemize}
\item \emph{T-array nodes:} Store current state $x_i$, receive bias $b_i$ and neighbor currents $\sum_j J_{ij} x_j$.
\item \emph{Measurement Plane:} Samples and holds $x_i(t_k)$ at discrete times; stores local register for $\Delta x_i^k$.
\item \emph{Local Statistics:} Computes $\langle x_i \rangle$, $\langle x_i x_j \rangle$, and velocity residuals $\delta_i^k = \Delta x_i^k - \text{predicted}$.
\item \emph{Gradient Engine:} Accumulates $\Delta b_i \propto \sum_k \delta_i^k$ and $\Delta J_{ij} \propto \sum_k \delta_i^k x_j^k$.
\item \emph{DAC banks:} Store $\{b_i\}$ and $\{J_{ij}\}$; dual-bank allows atomic swap.
\end{itemize}

\section{Finite-Time Learning Objectives and Gradient Estimation}
\label{sec:learning_rules}

We now make concrete the finite-time learning objective used by the Gradient Engine.
We work with a continuous state vector $\mathbf{x}\in\mathbb{R}^N$ and specialize the generic energy $U$ from Section~2 to a $\varphi^4$+Ising potential $V_{\bm{\theta}}(\mathbf{x})$ whose gradients take the form
\begin{equation}
\partial_i V_{\bm{\theta}}(\mathbf{x}) =
2J_2 x_i + 4J_4 x_i^3 + b_i + \sum_{j\in\mathcal{N}(i)} J_{ij} x_j,
\label{eq:local_potential_grad}
\end{equation}
where $J_2$ and $J_4$ are scalar coefficients, $b_i$ are local biases, $J_{ij}$ are
couplings on the interaction graph, and $\mathcal{N}(i)$ denotes the neighbour set of
node $i$.  The overdamped dynamics can then be written as
\begin{equation}
dx_i = -\mu\,\partial_i V_{\bm{\theta}}(\mathbf{x})\,dt
      + \sqrt{2k_{\rm B}T\mu}\;dW_i(t),
\end{equation}
with mobility $\mu$ and thermal energy $k_{\rm B}T$.

Discretising time with step size $\Delta t$ and using an Euler--Maruyama scheme gives
the observed forward trajectory
\begin{equation}
x_i^{k+1} = x_i^{k} + \Delta x_i^{k},
\qquad
\Delta x_i^{k} = -\mu\,\partial_i V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t
 + \sqrt{2k_{\rm B}T\mu\,\Delta t}\;\eta_i^{k},
\end{equation}
where $\mathbf{x}^{\prime k}$ is the evaluation point for the drift and $\eta_i^{k}$ are i.i.d.\ standard normal variables.
For simplicity we use $\mathbf{x}^{\prime k} = \mathbf{x}^k$ (the explicit Euler scheme), though midpoint or implicit schemes can reduce discretization bias at the cost of requiring $\mathbf{x}^{k+1}$ in the gradient expression.
The corresponding single-step transition density
$\tilde P_{\bm{\theta}}^{\rm step}(\Delta\mathbf{x}^k\mid\mathbf{x}^k)$ is Gaussian in
$\Delta\mathbf{x}^k$.

Our learning objective at observation times $\{t_k\}$ is the negative log-likelihood
of these observed forward steps,
\begin{equation}
L(\bm{\theta}) = -\sum_{k}
\ln \tilde P_{\bm{\theta}}^{\rm step}\!\left(\Delta\mathbf{x}^k\mid\mathbf{x}^k\right),
\label{eq:finite_time_loss}
\end{equation}
where $\Delta\mathbf{x}^k = \mathbf{x}^{k+1}-\mathbf{x}^{k}$ is the displacement over step $k$.
Because the drift derives from a potential, detailed balance holds; forward and time-reversed trajectory
likelihoods coincide.  As a result, minimising (\ref{eq:finite_time_loss}) using
forward trajectories also maximises the likelihood of the most probable
backward (reconstructive) trajectories.

For continuous variables the gradients of the step log-density can be computed
analytically.  For a coupling $J_{ij}$ we obtain
\begin{equation}
\!-\,\frac{\partial}{\partial J_{ij}}
\ln \tilde P_{\bm{\theta}}^{\rm step}\!\left(\Delta\mathbf{x}^k\right)
 =
\frac{-\Delta x_i^{k} + \mu\,\partial_i V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t}
     {2k_{\rm B}T}\;x_j^{k}
 +
\frac{-\Delta x_j^{k} + \mu\,\partial_j V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t}
     {2k_{\rm B}T}\;x_i^{k},
\label{eq:J_grad}
\end{equation}
and for a bias $b_i$
\begin{equation}
\!-\,\frac{\partial}{\partial b_i}
\ln \tilde P_{\bm{\theta}}^{\rm step}\!\left(\Delta\mathbf{x}^k\right)
 =
\frac{-\Delta x_i^{k} + \mu\,\partial_i V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t}
     {2k_{\rm B}T}.
\label{eq:b_grad}
\end{equation}
Summing these contributions over timesteps $k$ and replicas yields unbiased
stochastic gradients of $L(\bm{\theta})$ with respect to all local parameters
$(b_i,J_{ij})$ using only quantities that are locally available on chip:
displacements $\Delta x_i^{k}$, instantaneous forces
$\partial_i V_{\bm{\theta}}$, and neighbour states $x_j^{k}$.

\subsection{Velocity Matching Interpretation}
The gradient formula (\ref{eq:J_grad}) admits a physical interpretation as \emph{velocity matching}.
Defining the observed velocity $\dot x_i \approx \Delta x_i^k / \Delta t$ and the predicted force $f_i(\mathbf{x}) = -\mu\,\partial_i V_{\bm{\theta}}(\mathbf{x})$, we can rewrite the update rule as
\begin{equation}
\Delta J_{ij} \propto \left( f_i(\mathbf{x}) - \dot x_i \right) x_j + \left( f_j(\mathbf{x}) - \dot x_j \right) x_i.
\label{eq:velocity_matching}
\end{equation}
The mismatch between predicted force and observed velocity provides the error signal.
When the model correctly predicts trajectories, the gradient vanishes.

This formulation connects to \emph{denoising score matching}: the score function $\nabla_{\mathbf{x}} \log p(\mathbf{x})$ equals $-\nabla V / k_{\rm B}T$ at equilibrium, so training the drift to match observed velocities is equivalent to learning the score.
However, a crucial distinction applies: score matching optimizes \emph{local} gradients near data points but does not constrain the \emph{global} energy landscape.
For multi-modal distributions, this implies that class-conditional parameterization may be necessary (see Appendix~\ref{sec:experiments}).

\subsection{Time-Reversal and Reconstruction}
Because the dynamics derive from a potential, detailed balance ensures that forward (relaxation) and backward (reconstruction) trajectory likelihoods coincide.
Starting from data $\mathbf{x}_{\rm data}$, thermal relaxation generates a forward trajectory toward higher entropy.
Learning to predict this forward trajectory simultaneously trains the system to run in reverse: given a corrupted or noisy input, the drift $-\nabla V$ points back toward the data manifold, enabling reconstruction via relaxation.

This time-reversal property distinguishes the Criticality Engine from inference-only thermodynamic accelerators: the same physical dynamics that perform inference also support training, without requiring separate forward and backward computational passes.

\textbf{Detailed balance as an estimation ansatz.}
We emphasize that detailed balance here is a \emph{modeling assumption}, not a claim about the physical substrate.
Real p-bit arrays will exhibit parasitics, asymmetries, and colored noise that violate exact time-reversibility.
However, we adopt Eqs.~(\ref{eq:J_grad})--(\ref{eq:b_grad}) as a \emph{surrogate generative model} for learning: from observed increments $\Delta x_i^k$ and local neighbor states $x_j(t_k)$, we obtain closed-form local gradients $\partial_{J_{ij}} \log P(\Delta \mathbf{x}^k | \mathbf{x}^k)$.
The backward model provides an additional consistency signal and reduces estimator variance.
Deviations from the assumed model appear as structured residuals (action asymmetry between forward and backward paths), which can be monitored as a diagnostic and potentially incorporated into more refined learning rules.

\section{Path Integral Formulation: Learning as Constraint Satisfaction}
\label{sec:constraint}

The finite-time learning objective of Section~\ref{sec:learning_rules} admits a deeper interpretation through the Onsager-Machlup path integral formulation.
This perspective reveals that learning is not gradient descent over an unbounded loss landscape, but rather \emph{constraint satisfaction} on a surface defined by physical consistency.

\subsection{The Onsager-Machlup Action}

For overdamped Langevin dynamics (\ref{eq:langevin}), the probability of observing a specific trajectory $\mathbf{x}(t)$ is given by the path integral measure
\begin{equation}
P[\mathbf{x}(t)] \propto \exp\left(-S[\mathbf{x}(t)]\right),
\label{eq:path_prob}
\end{equation}
where $S$ is the Onsager-Machlup action functional:
\begin{equation}
S[\mathbf{x}(t)] = \int_0^T \frac{\left(\dot{\mathbf{x}} + \mu\nabla V_{\bm{\theta}}(\mathbf{x})\right)^2}{4\mu k_{\rm B}T}\,dt.
\label{eq:OM_action}
\end{equation}
The action measures the squared ``velocity mismatch'' between the observed trajectory velocity $\dot{\mathbf{x}}$ and the predicted drift $-\mu\nabla V_{\bm{\theta}}$.

\textbf{Key observation:} When the model parameters $\bm{\theta}$ exactly match the true physical system, the predicted drift equals the expected drift, so the residual $\dot{\mathbf{x}} + \mu\nabla V_{\bm{\theta}}$ consists only of thermal noise.
In this case, the action $S$ is minimized (to a value set by the noise variance), and the trajectory probability $\exp(-S)$ is maximized.

\subsection{The Constraint Surface}

Define the \emph{constraint surface} $\mathcal{C}$ as the set of parameters for which observed trajectories have minimal action:
\begin{equation}
\mathcal{C} = \left\{ \bm{\theta} : S[\mathbf{x}_{\rm obs}(t); \bm{\theta}] = S_{\rm thermal} \right\},
\end{equation}
where $S_{\rm thermal} = \int_0^T (\text{noise})^2/(4\mu k_{\rm B}T)\,dt$ represents the irreducible action from thermal fluctuations.

On this surface, the action reaches its minimum (the ``thermal floor'' set by irreducible noise), and observed trajectories are \emph{maximally likely} relative to all competing paths.
Learning consists of projecting onto $\mathcal{C}$---finding parameters where the observed dynamics are physically consistent with the model.

This is fundamentally different from conventional machine learning optimization:
\begin{itemize}
\item \textbf{Conventional view:} Minimize a loss function over all possible parameter values. The optimum may be far from the starting point, requiring extensive search.
\item \textbf{Constraint view:} Project onto the surface where physics is self-consistent. The constraint is \emph{local}---each trajectory provides evidence about the correct parameters.
\end{itemize}

\subsection{Temperature as a Lagrangian Multiplier}

The action (\ref{eq:OM_action}) can be rewritten as a constrained optimization problem.
We seek parameters $\bm{\theta}$ that minimize the integrated velocity mismatch
\begin{equation}
\min_{\bm{\theta}} \int_0^T \left(\dot{\mathbf{x}} + \mu\nabla V_{\bm{\theta}}(\mathbf{x})\right)^2 dt
\end{equation}
subject to the constraint that trajectories match observed data.
The temperature $k_{\rm B}T$ enters as a \emph{Lagrangian multiplier} controlling the tightness of this constraint:
\begin{itemize}
\item Low $T$: Strict constraint. Parameters must explain trajectories almost deterministically. Small errors are heavily penalized.
\item High $T$: Soft constraint. Greater tolerance for deviations, allowing trajectories that would be improbable deterministically.
\end{itemize}

This reveals why operating near criticality is advantageous: critical fluctuations provide informative trajectories that strongly constrain the parameter space, while the temperature $T$ modulates the learning sensitivity.

\subsection{Forward-Backward Symmetry and Detailed Balance}

Under the potential-driven surrogate model, forward and backward trajectory likelihoods are related by detailed balance.
For any trajectory $\mathbf{x}(t)$ from $\mathbf{x}_0$ to $\mathbf{x}_T$, the time-reversed trajectory $\tilde{\mathbf{x}}(t) = \mathbf{x}(T-t)$ has action
\begin{equation}
S[\tilde{\mathbf{x}}] = S[\mathbf{x}] + \frac{V(\mathbf{x}_T) - V(\mathbf{x}_0)}{k_{\rm B}T}.
\label{eq:detailed_balance}
\end{equation}
At equilibrium, where starting and ending configurations are drawn from the same distribution, the average difference vanishes:
\begin{equation}
\langle S_{\rm forward} \rangle = \langle S_{\rm backward} \rangle.
\end{equation}

We use the backward-direction objective as an estimation constraint, not as an assumption that the underlying device obeys detailed balance.
In the potential-driven limit, this construction coincides with the usual reversible setting; outside that limit it remains a useful surrogate learning signal and diagnostic residual.
Any asymmetry between forward and backward action suggests the model is incomplete---either the learned parameters are wrong, or the drift has solenoidal components not captured by the gradient ansatz.

\subsection{Why Learning is Fast: Constraint Satisfaction vs. Search}

The constraint surface interpretation explains an empirical observation: trajectory-based learning converges remarkably fast compared to conventional methods.

\textbf{Numerical verification.}
Figure~\ref{fig:convergence} shows parameter recovery on a synthetic task: a ground-truth model generates trajectories, and a randomly-initialized model learns to match them.
Within 10--20 epochs, cosine similarity between learned and true parameters exceeds 99\%.
The action difference $|S_{\rm forward} - S_{\rm backward}|$ drops by orders of magnitude, confirming that detailed balance is restored.

\textbf{Experimental protocol.}
The verification uses: $N = 30$ dimensional state space; $\varphi^4$ + bias potential with $J_2 = -1$, $J_4 = 0.5$, $kT = 0.5$; ground-truth bias $\mathbf{b}^* \sim \mathcal{N}(0, 2^2)$; 1000 equilibrium samples as starting points (500-step burn-in at $\Delta t = 0.02$); 20-step trajectories per sample at $\Delta t = 0.05$; Adam optimizer with learning rate 0.1; batch size 100.
Results are consistent across 10 random seeds (mean cosine similarity $0.9977 \pm 0.002$ at epoch 100).

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{paper_figures/trajectory_action_learning.png}
\caption{Trajectory action learning converges rapidly. Top left: Forward and backward actions converge to the thermal floor. Top right: Action asymmetry (detailed balance violation) decreases exponentially. Bottom left: Cosine similarity with true parameters approaches unity. Bottom right: Scatter plot of learned vs. true bias parameters shows near-perfect recovery ($>$99\% correlation).}
\label{fig:convergence}
\end{figure}

This rapid convergence arises because:
\begin{enumerate}
\item \textbf{Strong gradient signal:} Each trajectory point provides information about local parameters. The gradient is not sparse but dense in time.
\item \textbf{Convexity near the solution:} The constraint surface is locally quadratic in the velocity mismatch, so gradient descent rapidly finds the projection.
\item \textbf{Detailed balance as regularization:} Forward-backward symmetry over-constrains the problem, reducing the effective parameter space.
\end{enumerate}

Conventional neural network training faces a search problem over a high-dimensional, non-convex landscape.
Trajectory-based learning faces a projection problem onto a constraint surface---geometrically simpler and faster to solve.

\subsection{Implications for Hardware}

The constraint-satisfaction view has direct hardware implications:
\begin{itemize}
\item \textbf{Few training epochs:} If 10--20 passes through the data suffice, on-chip learning becomes practical even with limited memory bandwidth.
\item \textbf{Local computation:} The action is a sum of local terms $(\Delta x_i - \text{predicted})^2$, each computable from quantities available at node $i$.
\item \textbf{Automatic stopping:} Training converges when $S \approx S_{\rm thermal}$. No external validation set is needed---physical consistency is the stopping criterion.
\end{itemize}

\section{Hardware Realization}
\label{sec:hardware}

The Criticality Engine requires no exotic materials or fabrication processes.
Every component can be built from standard CMOS primitives available in any modern foundry.
This section describes how to construct each subsystem, with reference to existing hardware demonstrations.
Figure~\ref{fig:circuits} shows the three key circuit elements.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{paper_figures/circuit_diagrams.png}
\caption{Circuit building blocks. (a) A stochastic p-bit cell: cross-coupled inverters biased at threshold fluctuate between states due to thermal noise. (b) Current mirror coupling: the W/L ratio sets the coupling strength $J_{ij}$. (c) Dual-bank reconfiguration: the MUX atomically swaps between active and shadow parameter banks.}
\label{fig:circuits}
\end{figure}

\subsection{The Stochastic Unit: P-bits from CMOS}

The fundamental building block is a \emph{probabilistic bit} (p-bit): a circuit element that fluctuates randomly between 0 and 1 with controllable bias.
Unlike deterministic digital logic, p-bits embrace noise as a computational resource.

\textbf{Cross-coupled inverters.}
The simplest p-bit is a pair of cross-coupled CMOS inverters (Figure~\ref{fig:circuits}a).
When biased precisely at threshold---where both inverters have equal drive strength---the circuit becomes metastable.
Thermal noise (Johnson-Nyquist fluctuations in the transistor channels) randomly kicks the output between the two stable states.

The physics is exactly the $\varphi^4$ double-well potential:
\begin{equation}
V(x) = -\frac{1}{2}|J_2| x^2 + \frac{1}{4}J_4 x^4,
\end{equation}
where $x$ represents the differential voltage between the two inverter outputs.
The barrier height between wells is set by transistor sizing; the fluctuation rate depends on temperature and device capacitance.

\textbf{Existing demonstrations.}
P-bits have been demonstrated in multiple technologies:
\begin{itemize}
\item \textbf{CMOS} (Purdue/Kerem Camsari group): 14nm FinFET p-bits achieving GHz fluctuation rates, used for combinatorial optimization and Boltzmann machine inference.
\item \textbf{MTJ-based} (Tohoku University): Magnetic tunnel junctions with stochastic switching, offering non-volatility and lower power.
\item \textbf{FPGA emulation}: Pseudo-random implementations for algorithm development, though lacking true thermal noise.
\end{itemize}

A key advantage of CMOS p-bits is \emph{tunable bias}: applying a small differential voltage shifts the switching probability, implementing the local bias $b_i$ in our energy function.

\subsection{The Coupling Fabric: Current Mirrors as Synapses}

Interactions between nodes require physical connections that implement the coupling weights $J_{ij}$.
Current mirrors provide an elegant solution (Figure~\ref{fig:circuits}b).

\textbf{How it works.}
A current mirror consists of two transistors sharing a common gate voltage.
When the input transistor (M1) conducts current $I_{\rm in}$, the output transistor (M2) produces current $I_{\rm out}$ scaled by the ratio of their W/L (width/length) values:
\begin{equation}
I_{\rm out} = \frac{(W/L)_2}{(W/L)_1} \cdot I_{\rm in} = J_{ij} \cdot I_{\rm in}.
\end{equation}

The coupling strength $J_{ij}$ is thus \emph{programmed by geometry}.
For a learning system, we need tunable couplings; this can be achieved via:
\begin{itemize}
\item \textbf{Transistor arrays}: Multiple parallel transistors that can be switched in/out to change effective W/L.
\item \textbf{Floating-gate devices}: Analog charge storage on an isolated gate, allowing continuous weight adjustment.
\item \textbf{Memristors}: Two-terminal devices whose resistance encodes the coupling strength.
\end{itemize}

\textbf{Signed couplings.}
Neural networks require both excitatory ($J_{ij} > 0$) and inhibitory ($J_{ij} < 0$) connections.
Differential current mirrors achieve this: two complementary paths carry currents representing positive and negative contributions, and the net effect on the target node is their difference.

\textbf{Scaling.}
For $N$ nodes with average connectivity $d$ (neighbors per node), the coupling fabric requires $O(Nd)$ mirror circuits.
Sparse connectivity---matching the locality of physical interactions---keeps this tractable even for large $N$.

\subsection{Measurement: Non-Destructive Trajectory Sampling}

Learning from trajectories requires observing the system state at multiple times without disturbing the dynamics.

\textbf{Sense amplifiers.}
High-impedance sense amplifiers couple capacitively to each node, sampling the analog voltage $x_i(t)$ without drawing significant current.
The sampled value is latched into a local register on a clock edge, capturing a snapshot of the entire array.

\textbf{Timing requirements.}
The sampling interval $\Delta t$ must be:
\begin{itemize}
\item \textbf{Short enough} to capture the trajectory: $\Delta t < \tau_{\rm corr}$ (correlation time of fluctuations).
\item \textbf{Long enough} for meaningful displacement: $\Delta t > \tau_{\rm step}$ (time for one Langevin step to accumulate).
\end{itemize}
For MHz p-bit dynamics, sampling at 10--100~MHz provides the right resolution.

\textbf{Analog-to-digital conversion.}
If downstream processing is digital, ADCs convert the sampled voltages.
However, much of the statistics computation (means, correlations) can be performed in the analog domain, reducing ADC requirements.

\textbf{What ``non-destructive'' means.}
By non-destructive we mean standard high-impedance analog readout: sensing $x_i(t_k)$ does not reset the node or inject a perturbation comparable to intrinsic thermal noise.
This is implemented via buffered sampling with bandwidth separation, so the measurement contributes negligible back-action relative to the modeled stochastic dynamics.
If probe capacitance or loading becomes non-negligible, it manifests as an additional noise or drift term that is detectable (via action asymmetry) and calibratable.

\subsection{Reconfiguration: Dual-Bank Parameter Updates}

The learning loop requires updating parameters without disrupting ongoing computation.
The dual-bank architecture (Figure~\ref{fig:circuits}c) solves this elegantly.

\textbf{Two parameter banks.}
The system maintains two complete copies of all parameters $(J_{ij}, b_i)$:
\begin{itemize}
\item The \textbf{active bank} drives the T-array during the current computation.
\item The \textbf{shadow bank} receives gradient updates from the learning engine.
\end{itemize}

\textbf{Atomic swap.}
When a learning step completes, a single control signal (SEL in Figure~\ref{fig:circuits}c) swaps the roles of the two banks.
The multiplexer switches instantaneously, so the T-array transitions from old to new parameters in one clock cycle.
There is no intermediate state where parameters are partially updated.

\textbf{DAC specifications.}
Each parameter is stored digitally (8--12 bits typical) and converted to analog via DACs.
Key specifications:
\begin{itemize}
\item \textbf{Resolution}: 8 bits provides 256 levels, sufficient for most learning tasks; 12 bits enables finer control for precision-sensitive applications.
\item \textbf{Settling time}: 10--100~ns for modern DACs, setting the maximum parameter update rate at 10--100~MHz.
\item \textbf{Area}: A 10-bit DAC occupies roughly 100--1000~$\mu$m$^2$ in mature CMOS nodes.
\end{itemize}

\subsection{Putting It Together: A Complete Node}

Figure~\ref{fig:node_anatomy} shows the anatomy of a single node in the Criticality Engine.
Each node integrates five functional blocks: the stochastic p-bit cell at its core, current mirror networks for bidirectional coupling with neighbors, a sample-and-hold circuit for state readout, and DACs for programmable parameters.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{paper_figures/node_anatomy.png}
\caption{\textbf{Anatomy of a single node.}
The p-bit cell (center) receives weighted currents from $d$ neighbors via current mirrors (left) and broadcasts its state to $d$ neighbors (right).
A sample-and-hold circuit captures the stochastic state $x_i(t)$ for digital readout, while a bias DAC sets the local field $b_i$.
For a 1000-node array with $d=10$ connectivity, the total transistor count remains comparable to a small microcontroller.}
\label{fig:node_anatomy}
\end{figure}

Table~\ref{tab:component_count} summarizes the component requirements for a 1000-node array with average connectivity $d=10$.
The total transistor count of $\sim$280K is comparable to a small microcontroller.
In a 28nm process, this fits in approximately 1--10~mm$^2$---a small chip by modern standards.

\begin{table}[t]
\centering
\caption{Component count for a 1000-node array with $d=10$ average connectivity.}
\label{tab:component_count}
\small
\begin{tabular}{lrrr}
\toprule
\textbf{Component} & \textbf{Count} & \textbf{Transistors/unit} & \textbf{Total} \\
\midrule
P-bit cells & 1,000 & $\sim$10 & 10K \\
Current mirrors & 10,000 & $\sim$4 & 40K \\
Sample-and-hold & 1,000 & $\sim$6 & 6K \\
Bias DACs & 1,000 & $\sim$20 & 20K \\
Coupling DACs & 10,000 & $\sim$20 & 200K \\
\midrule
\textbf{Total} & & & \textbf{$\sim$280K} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparison to Existing Hardware}

\textbf{vs.\ GPUs/TPUs.}
Graphics processors simulate neural networks digitally, consuming $\sim$100--400W for training.
A back-of-envelope estimate suggests the Criticality Engine could operate in the mW--W range (dominated by DAC and ADC power, not computation), though detailed power analysis awaits fabrication.
The potential speedup comes from eliminating simulation overhead: physics computes the Langevin update in one physical timestep, not thousands of floating-point operations.

\textbf{vs.\ neuromorphic chips} (Intel Loihi, IBM TrueNorth).
These implement spiking neural networks with digital neurons and deterministic dynamics.
The Criticality Engine differs in two key ways: (1) it uses continuous stochastic dynamics rather than discrete spikes, and (2) it supports on-chip learning via trajectory-based gradients rather than spike-timing rules.

\textbf{vs.\ analog accelerators} (Mythic, Syntiant).
Analog matrix-vector multipliers accelerate inference but typically don't support training.
The Criticality Engine's dual-bank architecture and local gradient computation enable full training on-chip.

\textbf{vs.\ quantum annealers} (D-Wave).
Quantum annealers also solve optimization via physical dynamics, but require cryogenic cooling and have limited connectivity.
P-bit arrays operate at room temperature with arbitrary programmable connectivity, offering a practical near-term alternative for many applications.

\section{Convergence and Relaxation Analysis}

Let $\lambda_{\min}$ denote the spectral gap of the drift Jacobian.
Mixing toward the $t_k$ distribution occurs at rate $\lambda_{\min}$:
\begin{equation}
\|\; p_{\bm{\theta}}^{(k)} - p_{\bm{\theta}}^\ast \;\|_{\rm TV}
\lesssim e^{-\lambda_{\min} t_k}.
\end{equation}
Accuracy improves exponentially in $t_k$, subject to noise variance and discretization of the measurement plane.

Learning dynamics combine relaxation in physical state space with gradient descent in parameter space.
Convergence to a stationary point of $L$ requires:
\begin{itemize}
    \item Lipschitz-continuous drift $f_i(\mathbf{x};\bm{\theta})$ in both $\mathbf{x}$ and $\bm{\theta}$,
    \item bounded gradient variance (ensured by sufficient replica count $R$),
    \item learning rate $\eta$ small enough that parameters change slowly relative to state relaxation.
\end{itemize}
The effective learning rate must satisfy $\eta \ll \lambda_{\min}^{-1}$ to maintain quasi-static parameter evolution.

\section{Applications}

The architecture supports several application domains:
\textbf{Classification}, where finite-time objectives train the T-array to develop class-dependent attractors;
\textbf{generative modeling}, where time-reversal trajectory likelihoods enable physical generative models; and
\textbf{linear algebra}, where continuous-variable instantiations recover thermodynamic formulations with equilibrium means encoding $A^{-1}b$.

\textbf{Online learning latency.}
A single training update requires: (i) relaxation time $\sim\lambda_{\min}^{-1}$ for state equilibration, (ii) measurement and gradient computation, and (iii) DAC settling for parameter update.
With MHz-scale p-bit dynamics and ns-scale DAC updates, the architecture supports $\sim$kHz--MHz update rates---online in the sense of per-sample updates, though not streaming real-time for high-bandwidth data.

\section{Related Work}
\label{sec:related}

Our approach builds on several lines of research in energy-based learning, thermodynamic computing, and local learning rules.
We clarify connections and distinctions to prior work.

\textbf{Boltzmann machines and contrastive learning.}
The Boltzmann machine~\cite{hinton1985boltzmann} introduced energy-based learning with a two-phase contrastive rule: compare ``clamped'' (data-driven) and ``free'' (model-driven) statistics.
Restricted Boltzmann Machines~\cite{smolensky1986rbm} and Contrastive Divergence~\cite{hinton2002cd} made this practical by limiting connectivity and truncating MCMC chains.
Our trajectory-based approach differs fundamentally: we observe \emph{finite-time dynamics} rather than equilibrium statistics, and gradients emerge from velocity residuals rather than state differences.

\textbf{Equilibrium Propagation.}
Scellier and Bengio~\cite{scellier2017ep} introduced Equilibrium Propagation (EP), showing that gradients in energy-based models can be computed by comparing two equilibria: a ``free'' fixed point and a ``nudged'' fixed point where outputs are weakly clamped to targets.
The gradient formula $\partial L/\partial \theta \propto \lim_{\beta\to 0}(\text{state}_\text{nudged} - \text{state}_\text{free})/\beta$ requires the system to equilibrate twice per gradient.
Recent work~\cite{wanjura2025quantumep} connects EP to Onsager reciprocal relations (1931)---the symmetry of linear response coefficients.

Our approach uses a \emph{different} result from Onsager's work: the Onsager-Machlup functional~\cite{onsager1953fluctuations} (1953), which gives the probability density over stochastic trajectories.
This yields single-pass learning without equilibration: we observe one trajectory and extract gradients from velocity residuals.
Table~\ref{tab:ep_comparison} summarizes the distinctions.

\begin{table}[h]
\centering
\caption{Comparison of Equilibrium Propagation and our trajectory-based approach.}
\label{tab:ep_comparison}
\small
\begin{tabular}{lcc}
\toprule
& \textbf{Equilibrium Propagation} & \textbf{Ours (O-M Action)} \\
\midrule
Onsager foundation & Reciprocity (1931) & O-M Functional (1953) \\
Core object & Linear response & Path probability \\
Learning phases & Two (free + nudged) & One (observation) \\
Equilibration & Required (Ã—2) & Not required \\
Gradient source & State difference & Velocity residuals \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Thermodynamic computing hardware.}
P-bit arrays~\cite{camsari2017pbit,borders2019pbit} implement stochastic Ising machines for combinatorial optimization.
Normal Computing's stochastic processing unit~\cite{aifer2024thermodynamic} performs linear algebra via equilibrium fluctuations.
Extropic's thermodynamic sampling units use Josephson junctions for energy-based sampling~\cite{extropic2024tsu}.
These systems focus on \emph{inference}; our contribution is closing the \emph{training} loop on-chip via trajectory-based gradients.

\textbf{Score matching and diffusion models.}
Denoising score matching~\cite{vincent2011dsm,song2019score} learns the score function $\nabla \log p(\mathbf{x})$ by matching denoising directions.
Our velocity-matching interpretation (Section~\ref{sec:learning_rules}) connects to score matching: at equilibrium, $\nabla V / k_{\rm B}T = -\nabla \log p$.
However, score matching optimizes local reconstruction without explicit trajectory likelihoods, while our Onsager-Machlup objective provides a global path-integral formulation with closed-form gradients.

\textbf{Stochastic thermodynamics of learning.}
Goldt and Seifert~\cite{goldt2017thermodynamics} analyzed learning through the lens of stochastic thermodynamics, showing that thermodynamic costs bound information acquisition.
Our constraint-satisfaction interpretation complements this: correct parameters minimize the Onsager-Machlup action to its thermal floor, making learning a projection onto physical consistency rather than optimization over a loss landscape.

\section{Discussion}

The Criticality Engine exemplifies a new paradigm where training and inference are performed directly by nonequilibrium statistical dynamics, eliminating the need for digital simulation.
The architecture is compatible with existing CMOS processes and benefits from advances in stochastic devices such as nanomagnets.

\textbf{Limitations.}
This paper presents an architecture; numerical validation on specific tasks remains future work.
Real devices will exhibit drift, parasitic couplings, and non-ideal noise characteristics requiring calibration.
The finite-time objective may not align with equilibrium properties for all tasks.

\textbf{Future directions.}
Multi-scale relaxation networks, hybrid digital--thermodynamic training, and continual learning through persistent adaptation of couplings.

\section{Conclusion}

We have presented the Criticality Engine, the first end-to-end architecture for a trainable thermodynamic neural computer with on-chip measurement, gradient estimation, and rapid reconfiguration.

The central theoretical contribution is the reframing of learning as \emph{constraint satisfaction} rather than optimization.
The Onsager-Machlup action $S[\mathbf{x}(t)]$ defines a constraint surface: correct parameters minimize the action to its thermal floor, making observed trajectories maximally likely while competing paths are exponentially suppressed.
Temperature acts as a Lagrangian multiplier controlling constraint tightness, while forward-backward symmetry (detailed balance) provides additional constraints that accelerate convergence.
This perspective suggests why trajectory-based learning converges rapidly---99\% parameter recovery in 10--20 epochs in our experiments---because projecting onto a constraint surface is geometrically simpler than searching a high-dimensional loss landscape.

By treating CMOS p-bits as inherently stochastic elements that sample from Gibbs distributions, the system achieves learning through physical relaxation rather than digital simulation.
The local structure of the action---a sum of terms each depending only on neighboring states and velocities---enables on-chip gradient computation without global communication.
This approach offers a route toward ultra-low-power, massively-parallel artificial intelligence systems where learning emerges from the physics itself.

\appendix

\section{Numerical Experiments}
\label{sec:experiments}

We validate the learning rules on MNIST digit reconstruction, using 14$\times$14 downsampled images (196 visible units).

\subsection{The $\varphi^4$ Potential and Transistor Bistability}
The $\varphi^4$ structure is not an architectural choice but a physical consequence of operating transistors at threshold.
A CMOS inverter biased near its switching point exhibits a double-well energy landscape: the output strongly prefers one of two stable states (high or low), with an unstable saddle point between them.
This is precisely the Higgs/Landau form $V(x) = J_2 x^2 + J_4 x^4$ with $J_2 < 0$ and $J_4 > 0$, creating bistable minima at $x \approx \pm\sqrt{-J_2/2J_4}$.

The bistability is essential for representing discrete structure.
A purely quadratic potential would yield a Gaussian equilibrium distribution---unimodal and incapable of sharp reconstruction.
Instead, the $\varphi^4$ nonlinearity enables each node to ``snap'' toward one of two states, producing high-contrast outputs where pixels settle into their stable attractors rather than averaging toward gray.

\subsection{Denoising via Relaxation}
Using the biases learned via trajectory estimation (Section~\ref{sec:recognition}), we can denoise corrupted digits through Langevin relaxation under the $\varphi^4$+bias potential.
Starting from noisy inputs $\tilde x = x_{\rm data} + \epsilon$, Langevin dynamics follows the learned drift $-\nabla V_c$ back toward the data manifold for class $c$.
Figure~\ref{fig:trajectory} shows a representative relaxation trajectory: starting from random noise, structure emerges as pixels settle into their bistable attractors shaped by the learned bias.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{paper_figures/trajectory_fast_learned.png}
\caption{Relaxation trajectory under the $\varphi^4$+bias potential with biases learned via trajectory estimation. Starting from random noise (t=0), the system converges toward the digit-3 attractor. Structure emerges by t=20--40 as the learned bias $\mathbf{b}_3$ guides pixels toward the digit template while the $\varphi^4$ potential pushes them into bistable wells.}
\label{fig:trajectory}
\end{figure}

\subsection{Multi-Modal Structure Requires Class-Conditional Parameters}
With a single shared potential, unconditional generation from random initialization produces scattered noise rather than digit patterns.
From the constraint satisfaction perspective of Section~\ref{sec:constraint}, this is not a failure of learning but a geometric fact: \emph{a single constraint surface cannot represent multiple modes}.

The MNIST distribution has 10 distinct modes (digit classes).
A shared potential $V(\mathbf{x})$ defines one constraint surface $\mathcal{C}$ where the action is minimized.
But each digit class requires its \emph{own} constraint surface---trajectories of 3's should be likely under $V_3$, not under $V_7$.
Attempting to find shared parameters that make \emph{all} digit trajectories likely is geometrically impossible; the constraint surfaces for different classes do not intersect.

\textbf{Comparison to RBMs.}
Restricted Boltzmann Machines handle multi-modality through hidden units:
\begin{equation}
p(\mathbf{v}) = \sum_{\mathbf{h}} \exp(-E(\mathbf{v},\mathbf{h})) / Z.
\end{equation}
Each hidden configuration $\mathbf{h}$ implicitly selects a different visible pattern.
Our model has no hidden units---the couplings $J_{ij}$ connect visible units directly---so modes must be created through \emph{explicit} class-conditional parameters.

This is actually an advantage: explicit conditioning gives direct control over which attractor the system targets, enabling both generation (``produce a 3'') and targeted denoising (``clean this image assuming it's a 3'').
RBMs cannot easily target a specific class without modification.

\subsection{Class-Conditional Potentials Restore Generation}
The solution is \emph{class-conditional biases}: for each class $c \in \{0,\ldots,9\}$, we learn a separate bias vector $\mathbf{b}_c$, giving
\begin{equation}
\partial_i V_c(\mathbf{x}) = 2J_2 x_i + 4J_4 x_i^3 + b_{c,i} + \sum_{j} J_{ij} x_j.
\end{equation}
The couplings $J_{ij}$ remain shared across classes (encoding common structure like edges and correlations), while each $\mathbf{b}_c$ creates a class-specific constraint surface $\mathcal{C}_c$.

From the path integral view: training with class-$c$ trajectories projects $\mathbf{b}_c$ onto the surface where those trajectories have $\exp(-S_c) \to 1$.
Each class has its own constraint surface, and the biases are learned independently via the trajectory action.

\textbf{Attractor visualization:} Starting from uniform gray ($x_i = 0$) and evolving under $V_c$, pixels organize into the pattern for digit $c$.
The learned biases $\mathbf{b}_c$ resemble digit templates---analogous to Hopfield stored patterns---and conditional generation produces recognizable digits.
The fast convergence demonstrated in Figure~\ref{fig:trajectory} applies equally when starting from gray: structure emerges in $\sim$10 steps and stabilizes by $\sim$20 steps.

\textbf{Wrong-label test:} Denoising a digit with an incorrect class label reshapes it toward the wrong class, confirming that labels act as latent mode selectors that activate distinct attractor basins.

\subsection{Recognition and the Complete Pipeline}

A key question arises: how does the system know which bias $\mathbf{b}_c$ to load for an unknown input?
The answer is that the learned potentials themselves provide classification.

\textbf{Energy-based recognition.}
Given input $\mathbf{x}$, evaluate the energy (or gradient magnitude) under each class potential:
\begin{equation}
c^* = \arg\min_c V_c(\mathbf{x}) \quad \text{or equivalently} \quad c^* = \arg\min_c \|\nabla V_c(\mathbf{x})\|^2.
\end{equation}
The class with lowest energy (or smallest gradient, indicating proximity to equilibrium) is selected.
This is precisely \emph{Hopfield associative memory}: the biases $\mathbf{b}_c$ act as stored patterns, and recognition finds the nearest attractor.

\textbf{Complete inference pipeline:}
\begin{enumerate}
\item \textbf{Input}: Present noisy or partial pattern $\mathbf{x}$ to the T-array.
\item \textbf{Recognize}: Evaluate $V_c(\mathbf{x})$ for all $c$ (parallelizable across bias banks). Select $c^* = \arg\min_c V_c(\mathbf{x})$.
\item \textbf{Load}: Reconfiguration fabric loads bias bank $\mathbf{b}_{c^*}$.
\item \textbf{Relax}: System evolves under $V_{c^*}$; output converges to clean reconstruction.
\end{enumerate}

In principle, this unifies classification and generation in a single energy-based framework.
Initial experiments with denoising score matching achieved only $\sim$37\% classification accuracy---better than the 10\% random baseline, but far from state-of-the-art.
However, a key insight dramatically improves this result: \textbf{trajectory-based estimation with data-dependent diffusion achieves 76\% test accuracy in a single epoch}.

\textbf{Why trajectory estimation outperforms score matching.}
Score matching learns to denoise by matching the local score $\nabla \log p(\mathbf{x})$ to a denoising direction.
This is inherently \emph{local}---it optimizes within-class reconstruction but provides no signal for between-class discrimination.
The potentials learned via score matching overlap significantly; some classes (e.g., ``1'') have broadly attractive basins that capture inputs from other classes.

Trajectory estimation differs fundamentally: it observes how pixel values \emph{evolve} under a data-dependent diffusion process, then uses the analytical gradient formulas (\ref{eq:J_grad})--(\ref{eq:b_grad}) to estimate parameters.
The key is that the diffusion must be \emph{data-dependent}: different digit classes produce different diffusion signatures because their spatial intensity patterns differ.

\textbf{Conservative pixel diffusion.}
We implement a diffusion process where pixels (treated as particles) exchange intensity with neighbors while conserving total mass:
\begin{equation}
\Delta x_i = \alpha \sum_{j \in \text{neighbors}} (x_j - x_i) + \text{(zero-sum noise)}.
\end{equation}
A digit ``3'' diffuses differently than a digit ``7'' because their spatial gradients differ.
The residual between observed diffusion and model-predicted drift (from the $\varphi^4$ potential) captures this class-specific structure.

\textbf{One-epoch learning.}
The gradient estimator (\ref{eq:b_grad}) accumulates residuals:
\begin{equation}
\Delta b_{c,i} \propto -\frac{1}{N_c} \sum_{\text{samples of class } c} \text{residual}_i = -\langle \text{diffusion signature} \rangle_c.
\end{equation}
After one pass through the data, each $\mathbf{b}_c$ becomes a \emph{template} of the average diffusion signature for class $c$---which is essentially the average digit shape.
Classification then reduces to template matching: $c^* = \arg\min_c V_c(\mathbf{x}) = \arg\min_c (\text{const} + \mathbf{b}_c \cdot \mathbf{x})$.

\textbf{Experimental results.}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Epoch} & $\|\mathbf{b}\|$ & \textbf{Accuracy} \\
\midrule
Before training & 0.00 & 10.0\% (random) \\
After 1 epoch & 0.06 & 79.4\% \\
After 100 epochs & 6.43 & 80.2\% \\
\bottomrule
\end{tabular}
\end{center}
The jump from 10\% to 79\% in a single epoch demonstrates that trajectory estimation extracts class structure extremely efficiently.
Additional epochs provide marginal improvement because the templates are already well-formed.

Figure~\ref{fig:biases} shows the learned biases $-\mathbf{b}_c$, which visibly resemble the digit templates for each class.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{paper_figures/learned_biases.png}
\caption{Learned class biases $-\mathbf{b}_c$ via trajectory estimation. Each panel shows the bias template for one digit class (0--9). Red (positive) values attract pixels toward white; blue (negative) toward black. The templates visibly encode digit shapes, explaining the 76\% test accuracy: recognition reduces to template matching $c^* = \arg\min_c V_c(\mathbf{x})$.}
\label{fig:biases}
\end{figure}

\subsection{Contrastive Learning: Further Separating Constraint Surfaces}

Trajectory estimation already achieves 76\% test accuracy by learning class templates from diffusion signatures.
However, this is still a \emph{generative} objective---the gradient estimator (\ref{eq:b_grad}) projects each $\mathbf{b}_c$ onto the constraint surface $\mathcal{C}_c$ where class-$c$ trajectories are maximally likely, but doesn't explicitly ensure that constraint surfaces for different classes are well-separated.
For applications requiring higher accuracy, a \emph{discriminative} term can push constraint surfaces apart.

The contrastive loss explicitly separates class energies:
\begin{equation}
L_{\rm contrastive} = \sum_c \left[ V_c(\mathbf{x}_c) - \frac{1}{K-1}\sum_{j \neq c} V_j(\mathbf{x}_c) \right],
\label{eq:contrastive}
\end{equation}
where $\mathbf{x}_c$ denotes a sample from class $c$. This pushes the correct class energy $V_c(\mathbf{x}_c)$ down while pushing incorrect class energies $V_j(\mathbf{x}_c)$ up.

\textbf{Learning rules from contrastive loss.}
Consider our potential with shared couplings $J_{ij}$ and class-specific biases $\mathbf{b}_c$:
\begin{equation}
V_c(\mathbf{x}) = J_2 \|\mathbf{x}\|^2 + J_4 \|\mathbf{x}\|^4_4 + \mathbf{b}_c \cdot \mathbf{x} + \tfrac{1}{2}\mathbf{x}^\top J \mathbf{x}.
\end{equation}
Computing gradients of (\ref{eq:contrastive}):

\emph{For shared couplings $J_{ij}$}: Since $\partial V_c / \partial J_{ij} = x_i x_j$ is independent of class, we have
\begin{equation}
\frac{\partial L_{\rm contrastive}}{\partial J_{ij}} = \sum_c \left[ x_{c,i} x_{c,j} - \frac{K-1}{K-1} x_{c,i} x_{c,j} \right] = 0.
\end{equation}
The contrastive loss \emph{does not affect} the shared couplings---the positive and negative terms cancel exactly.

\emph{For class biases $\mathbf{b}_c$}: Since $\partial V_c / \partial b_{c,i} = x_i$ and $b_c$ only appears in $V_c$:
\begin{align}
\frac{\partial L_{\rm contrastive}}{\partial b_{c,i}} &= x_{c,i}, \label{eq:hebb_pos}\\
\frac{\partial L_{\rm contrastive}}{\partial b_{j,i}} &= -\frac{x_{c,i}}{K-1} \quad (j \neq c). \label{eq:hebb_neg}
\end{align}
This yields a \textbf{Hebbian/anti-Hebbian learning rule}:
\begin{align}
\Delta b_{c,i} &\propto -x_{c,i} \quad \text{(pull correct class toward data)}, \\
\Delta b_{j,i} &\propto +\frac{x_{c,i}}{K-1} \quad \text{(push incorrect classes away)}.
\end{align}
Geometrically: the correct-class constraint surface $\mathcal{C}_c$ is pulled toward the data point, while incorrect-class surfaces $\mathcal{C}_{j\neq c}$ are pushed away.
This is analogous to Hopfield learning with explicit negative examples, or the positive/negative phase of contrastive divergence.

\textbf{Key insight:} The shared couplings $J_{ij}$ are invariant under contrastive learning (the positive and negative terms cancel).
Only the class-specific biases $\mathbf{b}_c$ change.
This means trajectory action learning (for $J_{ij}$) and contrastive learning (for $\mathbf{b}_c$) can be combined: the former learns shared structure, the latter learns discriminative boundaries.

\textbf{Detailed balance in class space.}
While Langevin dynamics provides detailed balance in \emph{configuration space} $\mathbf{x}$:
\begin{equation}
d\mathbf{x} = -\nabla V_c \, dt + \sqrt{2k_{\rm B}T}\, d\mathbf{W}, \quad p(\mathbf{x}|c) \propto e^{-V_c(\mathbf{x})/k_{\rm B}T},
\end{equation}
the contrastive objective provides detailed balance in \emph{class space}. Define
\begin{equation}
p(c|\mathbf{x}) = \frac{\exp(-V_c(\mathbf{x})/T)}{\sum_j \exp(-V_j(\mathbf{x})/T)},
\label{eq:class_softmax}
\end{equation}
a Boltzmann distribution over discrete class labels. At low temperature $T \to 0$, this concentrates on $c^* = \arg\min_c V_c(\mathbf{x})$. The contrastive loss ensures this ``classification dynamics'' has the correct fixed point: data from class $c$ should flow to class $c$.

\textbf{Joint distribution and RBM connection.}
The class-conditional structure reveals a connection to Restricted Boltzmann Machines. Consider the joint distribution
\begin{equation}
p(\mathbf{x}, c) \propto \exp\left(-V_c(\mathbf{x})/k_{\rm B}T\right).
\label{eq:joint}
\end{equation}
This treats $c$ as a \emph{one-hot hidden unit}---analogous to an RBM where the hidden layer has exactly one active unit selecting among $K$ energy functions. The marginal over $\mathbf{x}$ is
\begin{equation}
p(\mathbf{x}) = \sum_c p(\mathbf{x},c) \propto \sum_c \exp(-V_c(\mathbf{x})/k_{\rm B}T),
\end{equation}
creating an implicit mixture model without requiring explicit hidden-unit sampling during training.

\textbf{Alternating dynamics.}
The joint distribution (\ref{eq:joint}) suggests an alternating inference procedure satisfying detailed balance:
\begin{enumerate}
\item \emph{Langevin step in $\mathbf{x}$}: Given class $c$, evolve $\mathbf{x}$ under $-\nabla V_c$.
\item \emph{Gibbs step in $c$}: Given $\mathbf{x}$, resample $c \sim p(c|\mathbf{x})$ via (\ref{eq:class_softmax}).
\end{enumerate}
This provides a principled way to jointly sample configurations and class labels, unifying generation and classification in a single thermodynamic framework.

The learned class biases $-\mathbf{b}_c$ function as attractor templates analogous to Hopfield stored patterns, resembling smoothed digit templates where positive (negative) bias values attract pixels toward white (black).
The contrastive signal $-b_c + \frac{1}{K-1}\sum_{j\neq c} b_j$ reveals what makes each digit \emph{unique}---the discriminative features that separate it from other classes.

With trajectory estimation achieving 76\% test accuracy, the system already provides strong classification performance.
Contrastive fine-tuning may further improve accuracy toward state-of-the-art levels, but the one-epoch trajectory result demonstrates that the physics-based approach is remarkably efficient at extracting class structure.

\subsection{Hardware Interpretation}
The class-conditional structure maps directly to hardware: the reconfiguration fabric stores $K$ bias banks $\{\mathbf{b}_0, \ldots, \mathbf{b}_{K-1}\}$.
Recognition requires evaluating $K$ energies, which can be done in parallel using $K$ copies of the gradient computation circuit, followed by a winner-take-all selection.
Once $c^*$ is determined, the appropriate bank is loaded ($\sim$ns DAC settling), and the T-array relaxes to produce the output.
This provides a physical implementation of content-addressable memory with learned attractors.

\subsection{Quantum-Classical Correspondence}
The Onsager-Machlup action developed in Section~\ref{sec:constraint} reveals a deeper structure.
The Langevin dynamics in $d$ dimensions corresponds to a $(d{+}1)$-dimensional classical field theory, where time $t$ plays the role of an extra spatial dimension.
The trajectory probability $P[\mathbf{x}(t)] \propto \exp(-S[\mathbf{x}])$ with action (\ref{eq:OM_action}) defines this higher-dimensional system.

This correspondence illuminates why learning at criticality is efficient: operating p-bits at threshold corresponds to the critical point of the $(d{+}1)$-dimensional theory, where temporal correlations become long-range.
The ``strong coupling in time'' allows information to propagate efficiently across the time dimension, meaning that each trajectory strongly constrains model parameters.
This is the physical origin of the rapid convergence observed in Figure~\ref{fig:convergence}---criticality maximizes the information content per trajectory, and the constraint surface becomes maximally informative.

\begin{thebibliography}{99}

\bibitem{hinton1985boltzmann}
D.~H. Ackley, G.~E. Hinton, and T.~J. Sejnowski,
``A learning algorithm for Boltzmann machines,''
\emph{Cognitive Science}, vol.~9, no.~1, pp.~147--169, 1985.

\bibitem{smolensky1986rbm}
P.~Smolensky,
``Information processing in dynamical systems: Foundations of harmony theory,''
in \emph{Parallel Distributed Processing}, vol.~1, ch.~6, pp.~194--281, MIT Press, 1986.

\bibitem{hinton2002cd}
G.~E. Hinton,
``Training products of experts by minimizing contrastive divergence,''
\emph{Neural Computation}, vol.~14, no.~8, pp.~1771--1800, 2002.

\bibitem{scellier2017ep}
B.~Scellier and Y.~Bengio,
``Equilibrium propagation: Bridging the gap between energy-based models and backpropagation,''
\emph{Frontiers in Computational Neuroscience}, vol.~11, p.~24, 2017.

\bibitem{wanjura2025quantumep}
C.~C. Wanjura, F.~Marquardt, \emph{et al.},
``Quantum equilibrium propagation for efficient training of quantum systems based on Onsager reciprocity,''
\emph{Nature Communications}, vol.~16, p.~6595, 2025.

\bibitem{onsager1953fluctuations}
L.~Onsager and S.~Machlup,
``Fluctuations and irreversible processes,''
\emph{Physical Review}, vol.~91, no.~6, pp.~1505--1512, 1953.

\bibitem{camsari2017pbit}
K.~Y. Camsari, R.~Faria, B.~M. Sutton, and S.~Datta,
``Stochastic p-bits for invertible logic,''
\emph{Physical Review X}, vol.~7, no.~3, p.~031014, 2017.

\bibitem{borders2019pbit}
W.~A. Borders, A.~Z. Pervaiz, S.~Fukami, K.~Y. Camsari, H.~Ohno, and S.~Datta,
``Integer factorization using stochastic magnetic tunnel junctions,''
\emph{Nature}, vol.~573, pp.~390--393, 2019.

\bibitem{aifer2024thermodynamic}
M.~Aifer, K.~Deng, G.~Kol, F.~Sheldon, and P.~Bhattacharya,
``Thermodynamic computing system for AI applications,''
\emph{Nature Communications}, vol.~16, p.~3474, 2025.

\bibitem{extropic2024tsu}
G.~Verdon, T.~McCourt, E.~Genin, \emph{et al.},
``Thermodynamic computing: From zero to one,''
Extropic Technical Report, 2024.

\bibitem{vincent2011dsm}
P.~Vincent,
``A connection between score matching and denoising autoencoders,''
\emph{Neural Computation}, vol.~23, no.~7, pp.~1661--1674, 2011.

\bibitem{song2019score}
Y.~Song and S.~Ermon,
``Generative modeling by estimating gradients of the data distribution,''
\emph{Advances in Neural Information Processing Systems}, vol.~32, 2019.

\bibitem{goldt2017thermodynamics}
S.~Goldt and U.~Seifert,
``Stochastic thermodynamics of learning,''
\emph{Physical Review Letters}, vol.~118, no.~1, p.~010601, 2017.

\bibitem{hopfield1982neural}
J.~J. Hopfield,
``Neural networks and physical systems with emergent collective computational abilities,''
\emph{Proceedings of the National Academy of Sciences}, vol.~79, no.~8, pp.~2554--2558, 1982.

\bibitem{lecun2006ebm}
Y.~LeCun, S.~Chopra, R.~Hadsell, M.~Ranzato, and F.~Huang,
``A tutorial on energy-based learning,''
in \emph{Predicting Structured Data}, MIT Press, 2006.

\end{thebibliography}

\end{document}
