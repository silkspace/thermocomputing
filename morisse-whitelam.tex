\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,bm}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

\title{
\vspace{-1cm}
\bfseries
The Criticality Engine:\\
Online Learning in Thermodynamic Neural Computers
}

\author{
Alexander Morisse\footnote{Email: alex@morisse.ai} \qquad
Stephen Whitelam\footnote{Email: whitelam@lbl.gov}
\\
\small{\emph{Independent Research Collaboration}}\\
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We introduce the \emph{Criticality Engine}, a computational architecture that performs both inference and online learning by exploiting the intrinsic stochastic dynamics of thermodynamic systems.
The key theoretical insight is that learning in such systems is \emph{constraint satisfaction}, not optimization: the Onsager-Machlup action $S[\mathbf{x}(t)] = \int (\dot{\mathbf{x}} + \nabla V)^2 dt / 4\mu k_{\rm B}T$ defines a constraint surface where $\exp(-S) \to 1$ for correctly-parameterized dynamics.
Temperature acts as a Lagrangian multiplier controlling constraint tightness, and forward-backward trajectory symmetry (detailed balance) provides additional constraints that accelerate convergence.
This perspective explains why trajectory-based learning is fast: we demonstrate $>$99\% parameter recovery in 10--20 epochs, compared to thousands of epochs typical in conventional neural network training.
CMOS p-bits biased at threshold naturally implement these dynamics in hardware, and the learning rules decompose into local operations suitable for on-chip gradient estimation.
We present a hardware blueprint with measurement plane, local statistics layer, and dual-bank reconfiguration fabric, establishing a route toward trainable thermodynamic neural computers with potential for significant energy reduction.
\end{abstract}

\section{Introduction}

Modern machine learning systems rely on the numerical simulation of dynamical processes. 
Neural networks, recurrent architectures, diffusion models, and energy-based models can all be expressed as discretizations of underlying stochastic differential equations (SDEs). 
Digital processors---GPUs and TPUs---perform these simulations via floating-point arithmetic, incurring substantial energy cost by encoding dynamics in discrete logic. 
An alternative paradigm is to realize the dynamics directly in hardware: a \emph{thermodynamic computer}.

Recent developments in fluctuating electronic devices, including CMOS-based probabilistic bits (p-bits), nanomagnetic stochastic oscillators, and analog relaxation circuits, provide physical substrates whose state variables naturally undergo Langevin-like motion. 
Existing thermodynamic hardware accelerators (e.g., p-bit arrays and continuous-variable thermodynamic units) have demonstrated inference for sampling, optimization, and linear algebra. 
However, no current architecture supports \emph{training} on-chip---the ability to estimate gradients of task-specific objectives and modify physical couplings accordingly.

In this work we propose the \emph{Criticality Engine}, an architecture designed to close this gap.
The key insight is twofold.
First, CMOS p-bits biased at threshold are inherently critical: the probability $p(s_i=1) = \sigma(h_i)$ realizes Gibbs sampling in physics, not simulation.
Second, and more fundamentally, learning in such systems is \emph{constraint satisfaction}, not optimization.
The Onsager-Machlup action functional measures how well model parameters explain observed trajectories; when $\exp(-S) \to 1$, the observed dynamics are maximally likely under the model.
Temperature acts as a Lagrangian multiplier, and forward-backward symmetry (detailed balance) provides additional constraints.
This perspective explains why trajectory-based learning converges rapidly---in 10--20 epochs rather than thousands---because projecting onto a constraint surface is geometrically simpler than searching a high-dimensional loss landscape.

Our system treats the thermodynamic array as a continuous-time recurrent neural network, provides a measurement plane for sampling trajectories, implements gradient estimators based on local correlators, and reconfigures couplings using a dual-bank analog front-end.
Learning emerges from the relaxation dynamics themselves, not from numerical backpropagation.

\section{Physical Model}

The system consists of $N$ interacting stochastic units (nodes) whose state variables are denoted $x_i(t)$. 
Depending on implementation, $x_i$ may represent:
\begin{itemize}
    \item a fluctuating voltage or current (continuous variable),
    \item the metastable output of a CMOS p-bit (binary variable),
    \item the magnetization state of a superparamagnetic nanomagnet.
\end{itemize}

We model the dynamics by the overdamped Langevin equation
\begin{equation}
\frac{dx_i}{dt} = f_i(\mathbf{x};\bm{\theta}) + \sqrt{2D_i}\,\eta_i(t),
\label{eq:langevin}
\end{equation}
where $\bm{\theta}$ denotes tunable physical parameters (biases, couplings), $D_i$ encodes noise strength, and $\eta_i(t)$ are independent white-noise processes.

In many realizations the deterministic drift derives from an energy function:
\begin{equation}
f_i(\mathbf{x};\bm{\theta}) = -\frac{\partial U(\mathbf{x};\bm{\theta})}{\partial x_i},
\end{equation}
so that (\ref{eq:langevin}) defines a relaxation toward a thermodynamic equilibrium distribution
\begin{equation}
p_{\bm{\theta}}(\mathbf{x}) \propto \exp\!\left[-\beta U(\mathbf{x};\bm{\theta})\right].
\end{equation}

We emphasize the analogy with recurrent neural networks (RNNs): the mapping
\[
f(\mathbf{x}) \;\longleftrightarrow\; \text{hidden-state update in a continuous-time RNN},
\]
and
\[
U(\mathbf{x}) \;\longleftrightarrow\; \text{energy function of an energy-based model}.
\]
Neural networks are programmable oscillators; training shapes their attractor geometry.
Our architecture treats these structures as \emph{physical}, not virtual.

\section{Processor Architecture}

The Criticality Engine realizes learning through a closed loop: physical dynamics generate trajectories, measurements extract statistics, and parameter updates reshape the energy landscape.
Figure~\ref{fig:architecture} shows the five integrated subsystems and their data flow.
We describe each in turn, following a single learning cycle.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{architecture_schematic.png}
\caption{Processor architecture of the Criticality Engine. The Thermodynamic Array (blue) implements Langevin dynamics; the Measurement Plane (orange) samples trajectories; the Local Statistics Layer (purple) computes correlations; the Gradient Engine (green) derives parameter updates; and the Reconfiguration Fabric (red) applies new parameters atomically. Arrows indicate data flow through one learning cycle.}
\label{fig:architecture}
\end{figure}

\subsection{Thermodynamic Array (T-array)}

At the heart of the processor lies a physical substrate that \emph{is} the neural network---not a simulation of one.
The T-array consists of $N$ stochastic nodes whose state variables $x_i(t)$ evolve according to Eq.~(\ref{eq:langevin}).
Each node experiences:
\begin{itemize}
\item A \textbf{local bias} $b_i$ realized as a programmable current source,
\item \textbf{Pairwise couplings} $J_{ij}$ implemented via current mirrors that inject current proportional to neighboring states,
\item \textbf{Intrinsic noise} from thermal fluctuations at the operating temperature $T$.
\end{itemize}

The key insight is that CMOS transistors biased at threshold are inherently bistable (the $\varphi^4$ potential), and their thermal fluctuations provide the stochastic drive $\eta_i(t)$ for free.
No random number generator is needed; the physics itself samples from the Gibbs distribution.

For ensemble averaging, the T-array may be replicated $R$ times, with all replicas sharing the same parameters but evolving independently.
This provides $R$ parallel trajectory samples per learning step.

\subsection{Measurement Plane}

Learning requires observing trajectories, not just equilibrium samples.
The Measurement Plane is a high-impedance sampling network that captures snapshots of all node states at programmable times $\{t_0, t_1, \ldots, t_K\}$.

Each observation consists of:
\begin{equation}
\{x_i^{(r)}(t_k)\} \quad \text{for } i = 1,\ldots,N; \quad r = 1,\ldots,R; \quad k = 0,\ldots,K.
\end{equation}
Sample-and-hold circuits latch analog voltages into local registers with minimal perturbation to the ongoing dynamics.
The measurement bandwidth must exceed the correlation time $\tau_{\rm corr} \sim \lambda_{\min}^{-1}$ to capture independent samples.

Critically, the Measurement Plane observes \emph{displacements} $\Delta x_i^k = x_i(t_{k+1}) - x_i(t_k)$---the raw material for trajectory-based learning.

\subsection{Local Statistics Layer}

The gradient formulas of Section~\ref{sec:learning_rules} require only local quantities: displacements, forces, and pairwise correlations.
The Local Statistics Layer computes these on-chip using dedicated correlator blocks:
\begin{equation}
m_i(t_k) = \frac{1}{R}\sum_{r=1}^R x_i^{(r)}(t_k), \qquad
C_{ij}(t_k) = \frac{1}{R}\sum_{r=1}^R x_i^{(r)}(t_k) x_j^{(r)}(t_k).
\end{equation}

These are \emph{sufficient statistics} for gradient estimation: the mean $m_i$ enters the bias gradient, and the correlation $C_{ij}$ enters the coupling gradient.
Because each correlator operates on a local neighborhood, the computation is embarrassingly parallel and scales linearly with node count.

\subsection{Gradient Engine}

A lightweight digital core---as simple as a vector microcontroller---reads the statistics and computes parameter updates according to the learning rules:
\begin{align}
\Delta J_{ij} &\propto \text{(observed velocity)} - \text{(predicted velocity)}, \\
\Delta b_i &\propto \text{(velocity mismatch at node } i\text{)}.
\end{align}
The precise formulas are derived in Section~\ref{sec:learning_rules}; the key point is that they involve only quantities already computed by the Local Statistics Layer.

The Gradient Engine writes updated parameters to a \emph{shadow bank}---a separate memory that does not yet affect the T-array.
This separation is essential: it allows the T-array to continue evolving under stable parameters while updates are being computed.

\subsection{Reconfiguration Fabric}

The final stage closes the learning loop.
A dual-bank DAC (digital-to-analog converter) array holds two complete parameter sets: the \emph{active bank} currently driving the T-array, and the \emph{shadow bank} receiving updates from the Gradient Engine.

When an update cycle completes, a global bank-select signal atomically swaps the roles of the two banks.
This provides:
\begin{itemize}
\item \textbf{Glitch-free updates:} The T-array never sees partially-written parameters.
\item \textbf{Instant rollback:} If an update causes instability, the previous parameters remain in the shadow bank.
\item \textbf{Class-conditional switching:} For multi-class problems, separate bias banks $\{\mathbf{b}_0, \ldots, \mathbf{b}_{K-1}\}$ can be swapped to select different attractors (Section~\ref{sec:experiments}).
\end{itemize}

DAC settling times ($\sim$10--100~ns) set the minimum interval between parameter updates, enabling update rates of 10--100~MHz---fast enough for online learning.

\subsection{The Learning Cycle}

A complete learning cycle proceeds as follows:
\begin{enumerate}
\item \textbf{Initialize:} Load data sample $\mathbf{x}_{\rm data}$ into the T-array (optionally with added noise).
\item \textbf{Evolve:} Let the system relax under Langevin dynamics for time $T$.
\item \textbf{Measure:} The Measurement Plane captures the trajectory $\{\mathbf{x}(t_k)\}$.
\item \textbf{Compute:} Local Statistics Layer aggregates correlations; Gradient Engine computes $\Delta\bm{\theta}$.
\item \textbf{Update:} New parameters written to shadow bank; bank-select swaps active/shadow.
\item \textbf{Repeat:} Next data sample begins the cycle anew.
\end{enumerate}

The entire cycle can complete in microseconds, limited primarily by the relaxation time of the T-array.
With MHz-scale p-bit dynamics, this enables thousands of parameter updates per second---true online learning in hardware.

\section{Finite-Time Learning Objectives and Gradient Estimation}
\label{sec:learning_rules}

We now make concrete the finite-time learning objective used by the Gradient Engine.
We work with a continuous state vector $\mathbf{x}\in\mathbb{R}^N$ and define a local
potential $V_{\bm{\theta}}(\mathbf{x})$ whose gradients take the form
\begin{equation}
\partial_i V_{\bm{\theta}}(\mathbf{x}) =
2J_2 x_i + 4J_4 x_i^3 + b_i + \sum_{j\in\mathcal{N}(i)} J_{ij} x_j,
\label{eq:local_potential_grad}
\end{equation}
where $J_2$ and $J_4$ are scalar coefficients, $b_i$ are local biases, $J_{ij}$ are
couplings on the interaction graph, and $\mathcal{N}(i)$ denotes the neighbour set of
node $i$.  The overdamped dynamics can then be written as
\begin{equation}
dx_i = -\mu\,\partial_i V_{\bm{\theta}}(\mathbf{x})\,dt
      + \sqrt{2k_{\rm B}T\mu}\;dW_i(t),
\end{equation}
with mobility $\mu$ and thermal energy $k_{\rm B}T$.

Discretising time with step size $\Delta t$ and using an Euler--Maruyama scheme gives
the observed forward trajectory
\begin{equation}
x_i^{k+1} = x_i^{k} + \Delta x_i^{k},
\qquad
\Delta x_i^{k} = -\mu\,\partial_i V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t
 + \sqrt{2k_{\rm B}T\mu\,\Delta t}\;\eta_i^{k},
\end{equation}
where $\mathbf{x}^{\prime k}$ is an evaluation point (e.g.\ the updated state) and
$\eta_i^{k}$ are i.i.d.\ standard normal variables.
The corresponding single-step transition density
$\tilde P_{\bm{\theta}}^{\rm step}(\Delta\mathbf{x}^k\mid\mathbf{x}^k)$ is Gaussian in
$\Delta\mathbf{x}^k$.

Our learning objective at observation times $\{t_k\}$ is the negative log-likelihood
of these observed forward steps,
\begin{equation}
L(\bm{\theta}) = -\sum_{k} w_k
\ln \tilde P_{\bm{\theta}}^{\rm step}\!\left(\Delta\mathbf{x}^k\mid\mathbf{x}^k\right),
\label{eq:finite_time_loss}
\end{equation}
where $\Delta\mathbf{x}^k = \mathbf{x}^{k+1}-\mathbf{x}^{k}$ is the displacement over
step $k$ and $w_k$ are user-chosen weights.  Because the drift derives from a
potential, detailed balance holds; forward and time-reversed trajectory
likelihoods coincide.  As a result, minimising (\ref{eq:finite_time_loss}) using
forward trajectories also maximises the likelihood of the most probable
backward (reconstructive) trajectories.

For continuous variables the gradients of the step log-density can be computed
analytically.  For a coupling $J_{ij}$ we obtain
\begin{equation}
\!-\,\frac{\partial}{\partial J_{ij}}
\ln \tilde P_{\bm{\theta}}^{\rm step}\!\left(\Delta\mathbf{x}^k\right)
 =
\frac{-\Delta x_i^{k} + \mu\,\partial_i V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t}
     {2k_{\rm B}T}\;x_j^{k}
 +
\frac{-\Delta x_j^{k} + \mu\,\partial_j V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t}
     {2k_{\rm B}T}\;x_i^{k},
\label{eq:J_grad}
\end{equation}
and for a bias $b_i$
\begin{equation}
\!-\,\frac{\partial}{\partial b_i}
\ln \tilde P_{\bm{\theta}}^{\rm step}\!\left(\Delta\mathbf{x}^k\right)
 =
\frac{-\Delta x_i^{k} + \mu\,\partial_i V_{\bm{\theta}}(\mathbf{x}^{\prime k})\Delta t}
     {2k_{\rm B}T}.
\label{eq:b_grad}
\end{equation}
Summing these contributions over timesteps $k$ and replicas yields unbiased
stochastic gradients of $L(\bm{\theta})$ with respect to all local parameters
$(b_i,J_{ij})$ using only quantities that are locally available on chip:
displacements $\Delta x_i^{k}$, instantaneous forces
$\partial_i V_{\bm{\theta}}$, and neighbour states $x_j^{k}$.

\subsection{Velocity Matching Interpretation}
The gradient formula (\ref{eq:J_grad}) admits a physical interpretation as \emph{velocity matching}.
Defining the observed velocity $\dot x_i \approx \Delta x_i^k / \Delta t$ and the predicted force $f_i(\mathbf{x}) = -\mu\,\partial_i V_{\bm{\theta}}(\mathbf{x})$, we can rewrite the update rule as
\begin{equation}
\Delta J_{ij} \propto \left( f_i(\mathbf{x}) - \dot x_i \right) x_j + \left( f_j(\mathbf{x}) - \dot x_j \right) x_i.
\label{eq:velocity_matching}
\end{equation}
The mismatch between predicted force and observed velocity provides the error signal.
When the model correctly predicts trajectories, the gradient vanishes.

This formulation connects to \emph{denoising score matching}: the score function $\nabla_{\mathbf{x}} \log p(\mathbf{x})$ equals $-\nabla V / k_{\rm B}T$ at equilibrium, so training the drift to match observed velocities is equivalent to learning the score.
However, a crucial distinction applies: score matching optimizes \emph{local} gradients near data points but does not constrain the \emph{global} energy landscape.
For multi-modal distributions, this implies that class-conditional parameterization may be necessary (see Appendix~\ref{sec:experiments}).

\subsection{Time-Reversal and Reconstruction}
Because the dynamics derive from a potential, detailed balance ensures that forward (relaxation) and backward (reconstruction) trajectory likelihoods coincide.
Starting from data $\mathbf{x}_{\rm data}$, thermal relaxation generates a forward trajectory toward higher entropy.
Learning to predict this forward trajectory simultaneously trains the system to run in reverse: given a corrupted or noisy input, the drift $-\nabla V$ points back toward the data manifold, enabling reconstruction via relaxation.

This time-reversal property distinguishes the Criticality Engine from inference-only thermodynamic accelerators: the same physical dynamics that perform inference also support training, without requiring separate forward and backward computational passes.

\section{Path Integral Formulation: Learning as Constraint Satisfaction}
\label{sec:constraint}

The finite-time learning objective of Section~\ref{sec:learning_rules} admits a deeper interpretation through the Onsager-Machlup path integral formulation.
This perspective reveals that learning is not gradient descent over an unbounded loss landscape, but rather \emph{constraint satisfaction} on a surface defined by physical consistency.

\subsection{The Onsager-Machlup Action}

For overdamped Langevin dynamics (\ref{eq:langevin}), the probability of observing a specific trajectory $\mathbf{x}(t)$ is given by the path integral measure
\begin{equation}
P[\mathbf{x}(t)] \propto \exp\left(-S[\mathbf{x}(t)]\right),
\label{eq:path_prob}
\end{equation}
where $S$ is the Onsager-Machlup action functional:
\begin{equation}
S[\mathbf{x}(t)] = \int_0^T \frac{\left(\dot{\mathbf{x}} + \mu\nabla V_{\bm{\theta}}(\mathbf{x})\right)^2}{4\mu k_{\rm B}T}\,dt.
\label{eq:OM_action}
\end{equation}
The action measures the squared ``velocity mismatch'' between the observed trajectory velocity $\dot{\mathbf{x}}$ and the predicted drift $-\mu\nabla V_{\bm{\theta}}$.

\textbf{Key observation:} When the model parameters $\bm{\theta}$ exactly match the true physical system, the predicted drift equals the expected drift, so the residual $\dot{\mathbf{x}} + \mu\nabla V_{\bm{\theta}}$ consists only of thermal noise.
In this case, the action $S$ is minimized (to a value set by the noise variance), and the trajectory probability $\exp(-S)$ is maximized.

\subsection{The Constraint Surface}

Define the \emph{constraint surface} $\mathcal{C}$ as the set of parameters for which observed trajectories have minimal action:
\begin{equation}
\mathcal{C} = \left\{ \bm{\theta} : S[\mathbf{x}_{\rm obs}(t); \bm{\theta}] = S_{\rm thermal} \right\},
\end{equation}
where $S_{\rm thermal} = \int_0^T (\text{noise})^2/(4\mu k_{\rm B}T)\,dt$ represents the irreducible action from thermal fluctuations.

On this surface, $\exp(-S) \to 1$ (normalized appropriately): the observed trajectories are \emph{maximally likely} under the model.
Learning consists of projecting onto $\mathcal{C}$---finding parameters where the observed dynamics are physically consistent with the model.

This is fundamentally different from conventional machine learning optimization:
\begin{itemize}
\item \textbf{Conventional view:} Minimize a loss function over all possible parameter values. The optimum may be far from the starting point, requiring extensive search.
\item \textbf{Constraint view:} Project onto the surface where physics is self-consistent. The constraint is \emph{local}---each trajectory provides evidence about the correct parameters.
\end{itemize}

\subsection{Temperature as a Lagrangian Multiplier}

The action (\ref{eq:OM_action}) can be rewritten as a constrained optimization problem.
We seek parameters $\bm{\theta}$ that minimize the integrated velocity mismatch
\begin{equation}
\min_{\bm{\theta}} \int_0^T \left(\dot{\mathbf{x}} + \mu\nabla V_{\bm{\theta}}(\mathbf{x})\right)^2 dt
\end{equation}
subject to the constraint that trajectories match observed data.
The temperature $k_{\rm B}T$ enters as a \emph{Lagrangian multiplier} controlling the tightness of this constraint:
\begin{itemize}
\item Low $T$: Strict constraint. Parameters must explain trajectories almost deterministically. Small errors are heavily penalized.
\item High $T$: Soft constraint. Greater tolerance for deviations, allowing trajectories that would be improbable deterministically.
\end{itemize}

This reveals why operating near criticality is advantageous: critical fluctuations provide informative trajectories that strongly constrain the parameter space, while the temperature $T$ modulates the learning sensitivity.

\subsection{Forward-Backward Symmetry and Detailed Balance}

A crucial feature of potential-driven dynamics is that forward and backward trajectory likelihoods are related by detailed balance.
For any trajectory $\mathbf{x}(t)$ from $\mathbf{x}_0$ to $\mathbf{x}_T$, the time-reversed trajectory $\tilde{\mathbf{x}}(t) = \mathbf{x}(T-t)$ has action
\begin{equation}
S[\tilde{\mathbf{x}}] = S[\mathbf{x}] + \frac{V(\mathbf{x}_T) - V(\mathbf{x}_0)}{k_{\rm B}T}.
\label{eq:detailed_balance}
\end{equation}
At equilibrium, where starting and ending configurations are drawn from the same distribution, the average difference vanishes:
\begin{equation}
\langle S_{\rm forward} \rangle = \langle S_{\rm backward} \rangle.
\end{equation}

This symmetry provides an additional constraint: \emph{any asymmetry between forward and backward action indicates incorrect parameters.}
The learning rule (\ref{eq:velocity_matching}) can be understood as enforcing this symmetry---adjusting parameters until forward and backward paths are equally likely.

\subsection{Why Learning is Fast: Constraint Satisfaction vs. Search}

The constraint surface interpretation explains an empirical observation: trajectory-based learning converges remarkably fast compared to conventional methods.

\textbf{Numerical verification.}
Figure~\ref{fig:convergence} shows parameter recovery on a synthetic task: a ground-truth model generates trajectories, and a randomly-initialized model learns to match them.
Within 10--20 epochs, cosine similarity between learned and true parameters exceeds 99\%.
The action difference $|S_{\rm forward} - S_{\rm backward}|$ drops by orders of magnitude, confirming that detailed balance is restored.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{trajectory_action_learning.png}
\caption{Trajectory action learning converges rapidly. Top left: Forward and backward actions converge to the thermal floor. Top right: Action asymmetry (detailed balance violation) decreases exponentially. Bottom left: Cosine similarity with true parameters approaches unity. Bottom right: Scatter plot of learned vs. true bias parameters shows near-perfect recovery ($>$99\% correlation).}
\label{fig:convergence}
\end{figure}

This rapid convergence arises because:
\begin{enumerate}
\item \textbf{Strong gradient signal:} Each trajectory point provides information about local parameters. The gradient is not sparse but dense in time.
\item \textbf{Convexity near the solution:} The constraint surface is locally quadratic in the velocity mismatch, so gradient descent rapidly finds the projection.
\item \textbf{Detailed balance as regularization:} Forward-backward symmetry over-constrains the problem, reducing the effective parameter space.
\end{enumerate}

Conventional neural network training faces a search problem over a high-dimensional, non-convex landscape.
Trajectory-based learning faces a projection problem onto a constraint surface---geometrically simpler and faster to solve.

\subsection{Implications for Hardware}

The constraint-satisfaction view has direct hardware implications:
\begin{itemize}
\item \textbf{Few training epochs:} If 10--20 passes through the data suffice, on-chip learning becomes practical even with limited memory bandwidth.
\item \textbf{Local computation:} The action is a sum of local terms $(\Delta x_i - \text{predicted})^2$, each computable from quantities available at node $i$.
\item \textbf{Automatic stopping:} Training converges when $S \approx S_{\rm thermal}$. No external validation set is needed---physical consistency is the stopping criterion.
\end{itemize}

\section{Hardware Realization}

The Criticality Engine requires no exotic materials or fabrication processes.
Every component can be built from standard CMOS primitives available in any modern foundry.
This section describes how to construct each subsystem, with reference to existing hardware demonstrations.
Figure~\ref{fig:circuits} shows the three key circuit elements.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{circuit_diagrams.png}
\caption{Circuit building blocks. (a) A stochastic p-bit cell: cross-coupled inverters biased at threshold fluctuate between states due to thermal noise. (b) Current mirror coupling: the W/L ratio sets the coupling strength $J_{ij}$. (c) Dual-bank reconfiguration: the MUX atomically swaps between active and shadow parameter banks.}
\label{fig:circuits}
\end{figure}

\subsection{The Stochastic Unit: P-bits from CMOS}

The fundamental building block is a \emph{probabilistic bit} (p-bit): a circuit element that fluctuates randomly between 0 and 1 with controllable bias.
Unlike deterministic digital logic, p-bits embrace noise as a computational resource.

\textbf{Cross-coupled inverters.}
The simplest p-bit is a pair of cross-coupled CMOS inverters (Figure~\ref{fig:circuits}a).
When biased precisely at threshold---where both inverters have equal drive strength---the circuit becomes metastable.
Thermal noise (Johnson-Nyquist fluctuations in the transistor channels) randomly kicks the output between the two stable states.

The physics is exactly the $\varphi^4$ double-well potential:
\begin{equation}
V(x) = -\frac{1}{2}|J_2| x^2 + \frac{1}{4}J_4 x^4,
\end{equation}
where $x$ represents the differential voltage between the two inverter outputs.
The barrier height between wells is set by transistor sizing; the fluctuation rate depends on temperature and device capacitance.

\textbf{Existing demonstrations.}
P-bits have been demonstrated in multiple technologies:
\begin{itemize}
\item \textbf{CMOS} (Purdue/Kerem Camsari group): 14nm FinFET p-bits achieving GHz fluctuation rates, used for combinatorial optimization and Boltzmann machine inference.
\item \textbf{MTJ-based} (Tohoku University): Magnetic tunnel junctions with stochastic switching, offering non-volatility and lower power.
\item \textbf{FPGA emulation}: Pseudo-random implementations for algorithm development, though lacking true thermal noise.
\end{itemize}

A key advantage of CMOS p-bits is \emph{tunable bias}: applying a small differential voltage shifts the switching probability, implementing the local bias $b_i$ in our energy function.

\subsection{The Coupling Fabric: Current Mirrors as Synapses}

Interactions between nodes require physical connections that implement the coupling weights $J_{ij}$.
Current mirrors provide an elegant solution (Figure~\ref{fig:circuits}b).

\textbf{How it works.}
A current mirror consists of two transistors sharing a common gate voltage.
When the input transistor (M1) conducts current $I_{\rm in}$, the output transistor (M2) produces current $I_{\rm out}$ scaled by the ratio of their W/L (width/length) values:
\begin{equation}
I_{\rm out} = \frac{(W/L)_2}{(W/L)_1} \cdot I_{\rm in} = J_{ij} \cdot I_{\rm in}.
\end{equation}

The coupling strength $J_{ij}$ is thus \emph{programmed by geometry}.
For a learning system, we need tunable couplings; this can be achieved via:
\begin{itemize}
\item \textbf{Transistor arrays}: Multiple parallel transistors that can be switched in/out to change effective W/L.
\item \textbf{Floating-gate devices}: Analog charge storage on an isolated gate, allowing continuous weight adjustment.
\item \textbf{Memristors}: Two-terminal devices whose resistance encodes the coupling strength.
\end{itemize}

\textbf{Signed couplings.}
Neural networks require both excitatory ($J_{ij} > 0$) and inhibitory ($J_{ij} < 0$) connections.
Differential current mirrors achieve this: two complementary paths carry currents representing positive and negative contributions, and the net effect on the target node is their difference.

\textbf{Scaling.}
For $N$ nodes with average connectivity $d$ (neighbors per node), the coupling fabric requires $O(Nd)$ mirror circuits.
Sparse connectivity---matching the locality of physical interactions---keeps this tractable even for large $N$.

\subsection{Measurement: Non-Destructive Trajectory Sampling}

Learning from trajectories requires observing the system state at multiple times without disturbing the dynamics.

\textbf{Sense amplifiers.}
High-impedance sense amplifiers couple capacitively to each node, sampling the analog voltage $x_i(t)$ without drawing significant current.
The sampled value is latched into a local register on a clock edge, capturing a snapshot of the entire array.

\textbf{Timing requirements.}
The sampling interval $\Delta t$ must be:
\begin{itemize}
\item \textbf{Short enough} to capture the trajectory: $\Delta t < \tau_{\rm corr}$ (correlation time of fluctuations).
\item \textbf{Long enough} for meaningful displacement: $\Delta t > \tau_{\rm step}$ (time for one Langevin step to accumulate).
\end{itemize}
For MHz p-bit dynamics, sampling at 10--100~MHz provides the right resolution.

\textbf{Analog-to-digital conversion.}
If downstream processing is digital, ADCs convert the sampled voltages.
However, much of the statistics computation (means, correlations) can be performed in the analog domain, reducing ADC requirements.

\subsection{Reconfiguration: Dual-Bank Parameter Updates}

The learning loop requires updating parameters without disrupting ongoing computation.
The dual-bank architecture (Figure~\ref{fig:circuits}c) solves this elegantly.

\textbf{Two parameter banks.}
The system maintains two complete copies of all parameters $(J_{ij}, b_i)$:
\begin{itemize}
\item The \textbf{active bank} drives the T-array during the current computation.
\item The \textbf{shadow bank} receives gradient updates from the learning engine.
\end{itemize}

\textbf{Atomic swap.}
When a learning step completes, a single control signal (SEL in Figure~\ref{fig:circuits}c) swaps the roles of the two banks.
The multiplexer switches instantaneously, so the T-array transitions from old to new parameters in one clock cycle.
There is no intermediate state where parameters are partially updated.

\textbf{DAC specifications.}
Each parameter is stored digitally (8--12 bits typical) and converted to analog via DACs.
Key specifications:
\begin{itemize}
\item \textbf{Resolution}: 8 bits provides 256 levels, sufficient for most learning tasks; 12 bits enables finer control for precision-sensitive applications.
\item \textbf{Settling time}: 10--100~ns for modern DACs, setting the maximum parameter update rate at 10--100~MHz.
\item \textbf{Area}: A 10-bit DAC occupies roughly 100--1000~$\mu$m$^2$ in mature CMOS nodes.
\end{itemize}

\subsection{Putting It Together: A Complete Node}

A single node in the Criticality Engine integrates:
\begin{enumerate}
\item One p-bit cell (cross-coupled inverters with bias input),
\item $d$ current mirror inputs (from neighboring nodes),
\item $d$ current mirror outputs (to neighboring nodes),
\item One sample-and-hold circuit with local register,
\item One DAC for the bias $b_i$, plus shared access to coupling DACs.
\end{enumerate}

For a 1000-node array with $d=10$ average connectivity, this amounts to roughly:
\begin{itemize}
\item 1000 p-bit cells ($\sim$10 transistors each),
\item 10,000 current mirrors ($\sim$4 transistors each),
\item 1000 sample-and-hold circuits,
\item 1000 bias DACs + 10,000 coupling DACs (or shared with time-multiplexing).
\end{itemize}
Total transistor count: $\sim$100K--1M, comparable to a small microcontroller.
In a 28nm process, this fits in approximately 1--10~mm$^2$---a small chip by modern standards.

\subsection{Comparison to Existing Hardware}

\textbf{vs.\ GPUs/TPUs.}
Graphics processors simulate neural networks digitally, consuming $\sim$100--400W for training.
The Criticality Engine implements the same dynamics physically, with estimated power in the mW--W range (dominated by DAC and ADC power, not computation).
The speedup comes from eliminating the simulation overhead: physics computes the Langevin update in one physical timestep, not thousands of floating-point operations.

\textbf{vs.\ neuromorphic chips} (Intel Loihi, IBM TrueNorth).
These implement spiking neural networks with digital neurons and deterministic dynamics.
The Criticality Engine differs in two key ways: (1) it uses continuous stochastic dynamics rather than discrete spikes, and (2) it supports on-chip learning via trajectory-based gradients rather than spike-timing rules.

\textbf{vs.\ analog accelerators} (Mythic, Syntiant).
Analog matrix-vector multipliers accelerate inference but typically don't support training.
The Criticality Engine's dual-bank architecture and local gradient computation enable full training on-chip.

\textbf{vs.\ quantum annealers} (D-Wave).
Quantum annealers also solve optimization via physical dynamics, but require cryogenic cooling and have limited connectivity.
P-bit arrays operate at room temperature with arbitrary programmable connectivity, offering a practical near-term alternative for many applications.

\section{Convergence and Relaxation Analysis}

Let $\lambda_{\min}$ denote the spectral gap of the drift Jacobian.
Mixing toward the $t_k$ distribution occurs at rate $\lambda_{\min}$:
\begin{equation}
\|\; p_{\bm{\theta}}^{(k)} - p_{\bm{\theta}}^\ast \;\|_{\rm TV}
\lesssim e^{-\lambda_{\min} t_k}.
\end{equation}
Accuracy improves exponentially in $t_k$, subject to noise variance and discretization of the measurement plane.

Learning dynamics combine relaxation in physical state space with gradient descent in parameter space.
Convergence to a stationary point of $L$ requires:
\begin{itemize}
    \item Lipschitz-continuous drift $f_i(\mathbf{x};\bm{\theta})$ in both $\mathbf{x}$ and $\bm{\theta}$,
    \item bounded gradient variance (ensured by sufficient replica count $R$),
    \item learning rate $\eta$ small enough that parameters change slowly relative to state relaxation.
\end{itemize}
The effective learning rate must satisfy $\eta \ll \lambda_{\min}^{-1}$ to maintain quasi-static parameter evolution.

\section{Applications}

The architecture supports several application domains:
\textbf{Classification}, where finite-time objectives train the T-array to develop class-dependent attractors;
\textbf{generative modeling}, where time-reversal trajectory likelihoods enable physical generative models; and
\textbf{linear algebra}, where continuous-variable instantiations recover thermodynamic formulations with equilibrium means encoding $A^{-1}b$.

\textbf{Online learning latency.}
A single training update requires: (i) relaxation time $\sim\lambda_{\min}^{-1}$ for state equilibration, (ii) measurement and gradient computation, and (iii) DAC settling for parameter update.
With MHz-scale p-bit dynamics and ns-scale DAC updates, the architecture supports $\sim$kHz--MHz update rates---online in the sense of per-sample updates, though not streaming real-time for high-bandwidth data.

\section{Discussion}

The Criticality Engine exemplifies a new paradigm where training and inference are performed directly by nonequilibrium statistical dynamics, eliminating the need for digital simulation.
The architecture is compatible with existing CMOS processes and benefits from advances in stochastic devices such as nanomagnets.

\textbf{Relation to prior work.}
The finite-time learning objective differs from classical Boltzmann machine training, which requires equilibrium sampling.
Our approach shares motivation with equilibrium propagation but provides explicit hardware primitives rather than algorithmic recipes.
Unlike inference-only thermodynamic accelerators (Ising machines, p-bit arrays for optimization), the Criticality Engine closes the training loop on-chip.

\textbf{Limitations.}
This paper presents an architecture; numerical validation on specific tasks remains future work.
Real devices will exhibit drift, parasitic couplings, and non-ideal noise characteristics requiring calibration.
The finite-time objective may not align with equilibrium properties for all tasks.

\textbf{Future directions.}
Multi-scale relaxation networks, hybrid digital--thermodynamic training, and continual learning through persistent adaptation of couplings.

\section{Conclusion}

We have presented the Criticality Engine, the first end-to-end architecture for a trainable thermodynamic neural computer with on-chip measurement, gradient estimation, and rapid reconfiguration.

The central theoretical contribution is the reframing of learning as \emph{constraint satisfaction} rather than optimization.
The Onsager-Machlup action $S[\mathbf{x}(t)]$ defines a constraint surface: when parameters are correct, $\exp(-S) \to 1$ and observed trajectories are maximally likely.
Temperature acts as a Lagrangian multiplier controlling constraint tightness, while forward-backward symmetry (detailed balance) provides additional constraints that accelerate convergence.
This explains the empirically observed rapid learning---99\% parameter recovery in 10--20 epochs---because projecting onto a constraint surface is geometrically simpler than searching a high-dimensional loss landscape.

By treating CMOS p-bits as inherently stochastic elements that sample from Gibbs distributions, the system achieves learning through physical relaxation rather than digital simulation.
The local structure of the action---a sum of terms each depending only on neighboring states and velocities---enables on-chip gradient computation without global communication.
This approach offers a route toward ultra-low-power, massively-parallel artificial intelligence systems where learning emerges from the physics itself.

\appendix

\section{Numerical Experiments}
\label{sec:experiments}

We validate the learning rules on MNIST digit reconstruction, using 14$\times$14 downsampled images (196 visible units).

\subsection{The $\varphi^4$ Potential and Transistor Bistability}
The $\varphi^4$ structure is not an architectural choice but a physical consequence of operating transistors at threshold.
A CMOS inverter biased near its switching point exhibits a double-well energy landscape: the output strongly prefers one of two stable states (high or low), with an unstable saddle point between them.
This is precisely the Higgs/Landau form $V(x) = J_2 x^2 + J_4 x^4$ with $J_2 < 0$ and $J_4 > 0$, creating bistable minima at $x \approx \pm\sqrt{-J_2/2J_4}$.

The bistability is essential for representing discrete structure.
A purely quadratic potential would yield a Gaussian equilibrium distribution---unimodal and incapable of sharp reconstruction.
Instead, the $\varphi^4$ nonlinearity enables each node to ``snap'' toward one of two states, producing high-contrast outputs where pixels settle into their stable attractors rather than averaging toward gray.

\subsection{Denoising via Relaxation}
Training the $\varphi^4$+Ising potential (\ref{eq:local_potential_grad}) with denoising score matching yields a system where corrupted digits relax toward clean reconstructions.
Starting from noisy inputs $\tilde x = x_{\rm data} + \epsilon$, Langevin dynamics follows the learned drift $-\nabla V$ back toward the data manifold.
Score alignment (cosine similarity between learned drift and direction toward clean data) exceeds 0.8, confirming that the learned potential encodes the data distribution.
Figure~\ref{fig:trajectory} shows a representative relaxation trajectory: a noisy digit ``3'' progressively sharpens as pixels settle into their bistable attractors.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{criticality_trajectory.png}
\caption{Relaxation trajectory under the class-conditional $\varphi^4$+Ising potential. Starting from heavy noise (t=0, barely recognizable), the system evolves under the digit-3 potential $V_3(\mathbf{x})$. Pixels are progressively ``attracted'' to the digit-3 attractor basin, crystallizing into a sharp reconstruction by t=300.}
\label{fig:trajectory}
\end{figure}

\subsection{Multi-Modal Structure Requires Class-Conditional Parameters}
With a single shared potential, unconditional generation from random initialization produces scattered noise rather than digit patterns.
From the constraint satisfaction perspective of Section~\ref{sec:constraint}, this is not a failure of learning but a geometric fact: \emph{a single constraint surface cannot represent multiple modes}.

The MNIST distribution has 10 distinct modes (digit classes).
A shared potential $V(\mathbf{x})$ defines one constraint surface $\mathcal{C}$ where $\exp(-S) \to 1$.
But each digit class requires its \emph{own} constraint surface---trajectories of 3's should be likely under $V_3$, not under $V_7$.
Attempting to find shared parameters that make \emph{all} digit trajectories likely is geometrically impossible; the constraint surfaces for different classes do not intersect.

\textbf{Comparison to RBMs.}
Restricted Boltzmann Machines handle multi-modality through hidden units:
\begin{equation}
p(\mathbf{v}) = \sum_{\mathbf{h}} \exp(-E(\mathbf{v},\mathbf{h})) / Z.
\end{equation}
Each hidden configuration $\mathbf{h}$ implicitly selects a different visible pattern.
Our model has no hidden units---the couplings $J_{ij}$ connect visible units directly---so modes must be created through \emph{explicit} class-conditional parameters.

This is actually an advantage: explicit conditioning gives direct control over which attractor the system targets, enabling both generation (``produce a 3'') and targeted denoising (``clean this image assuming it's a 3'').
RBMs cannot easily target a specific class without modification.

\subsection{Class-Conditional Potentials Restore Generation}
The solution is \emph{class-conditional biases}: for each class $k \in \{0,\ldots,9\}$, we learn a separate bias vector $\mathbf{b}_k$, giving
\begin{equation}
\partial_i V_k(\mathbf{x}) = 2J_2 x_i + 4J_4 x_i^3 + b_{k,i} + \sum_{j} J_{ij} x_j.
\end{equation}
The couplings $J_{ij}$ remain shared across classes (encoding common structure like edges and correlations), while each $\mathbf{b}_k$ creates a class-specific constraint surface $\mathcal{C}_k$.

From the path integral view: training with class-$k$ trajectories projects $\mathbf{b}_k$ onto the surface where those trajectories have $\exp(-S_k) \to 1$.
Each class has its own constraint surface, and the biases are learned independently via the trajectory action.

\textbf{Attractor visualization:} Starting from uniform gray ($x_i = 0$) and evolving under $V_k$, pixels organize into the pattern for digit $k$ (Figure~\ref{fig:attractors}).
The learned biases $\mathbf{b}_k$ resemble digit templates---analogous to Hopfield stored patterns---and conditional generation produces recognizable digits.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{attractors_comparison.png}
\caption{Comparison of real digit averages (top), learned attractors from relaxation under $V_k$ (middle), and raw bias templates $-\mathbf{b}_k$ (bottom). The learned attractors are sharper than real averages due to bistable $\varphi^4$ dynamics.}
\label{fig:attractors}
\end{figure}

\textbf{Wrong-label test:} Denoising a digit with an incorrect class label reshapes it toward the wrong class (Figure~\ref{fig:conditional}), confirming that labels act as latent mode selectors that activate distinct attractor basins.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{full_pipeline.png}
\caption{Full inference pipeline. Row 1: clean digits. Row 2: noisy inputs. Row 3: denoised using energy-predicted class $k^*$ (green = correct, red = incorrect prediction). Row 4: denoised using true class label. Classification accuracy is limited, but conditional denoising (row 4) is reliable.}
\label{fig:conditional}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{attractors_from_gray.png}
\caption{Attractor dynamics: each row shows evolution under a different class potential $V_k$. Starting from uniform gray (t=0), pixels self-organize into the corresponding digit pattern. This demonstrates that the learned biases create digit-specific energy basins.}
\label{fig:attractor_dynamics}
\end{figure}

\subsection{Recognition and the Complete Pipeline}

A key question arises: how does the system know which bias $\mathbf{b}_k$ to load for an unknown input?
The answer is that the learned potentials themselves provide classification.

\textbf{Energy-based recognition.}
Given input $\mathbf{x}$, evaluate the energy (or gradient magnitude) under each class potential:
\begin{equation}
k^* = \arg\min_k V_k(\mathbf{x}) \quad \text{or equivalently} \quad k^* = \arg\min_k \|\nabla V_k(\mathbf{x})\|^2.
\end{equation}
The class with lowest energy (or smallest gradient, indicating proximity to equilibrium) is selected.
This is precisely \emph{Hopfield associative memory}: the biases $\mathbf{b}_k$ act as stored patterns, and recognition finds the nearest attractor.

\textbf{Complete inference pipeline:}
\begin{enumerate}
\item \textbf{Input}: Present noisy or partial pattern $\mathbf{x}$ to the T-array.
\item \textbf{Recognize}: Evaluate $V_k(\mathbf{x})$ for all $k$ (parallelizable across bias banks). Select $k^* = \arg\min_k V_k(\mathbf{x})$.
\item \textbf{Load}: Reconfiguration fabric loads bias bank $\mathbf{b}_{k^*}$.
\item \textbf{Relax}: System evolves under $V_{k^*}$; output converges to clean reconstruction.
\end{enumerate}

In principle, this unifies classification and generation in a single energy-based framework.
However, our experiments reveal a limitation: biases trained via denoising score matching achieve only $\sim$37\% classification accuracy (vs.\ 10\% random baseline).
The potentials overlap significantly---some classes (e.g., ``1'') have broadly attractive basins that capture inputs from other classes.

This occurs because score matching optimizes \emph{within-class} reconstruction, not \emph{between-class} discrimination.

\subsection{Contrastive Learning: Separating Constraint Surfaces}

The trajectory action (Section~\ref{sec:constraint}) learns each $\mathbf{b}_k$ by projecting onto the constraint surface $\mathcal{C}_k$ where class-$k$ trajectories are maximally likely.
But this is a \emph{generative} objective---it doesn't ensure that constraint surfaces for different classes are well-separated.
For robust classification, we need a \emph{discriminative} term that pushes constraint surfaces apart.

The contrastive loss explicitly separates class energies:
\begin{equation}
L_{\rm contrastive} = \sum_k \left[ V_k(\mathbf{x}_k) - \frac{1}{K-1}\sum_{j \neq k} V_j(\mathbf{x}_k) \right],
\label{eq:contrastive}
\end{equation}
where $\mathbf{x}_k$ denotes a sample from class $k$. This pushes the correct class energy $V_k(\mathbf{x}_k)$ down while pushing incorrect class energies $V_j(\mathbf{x}_k)$ up.

\textbf{Learning rules from contrastive loss.}
Consider our potential with shared couplings $J_{ij}$ and class-specific biases $\mathbf{b}_k$:
\begin{equation}
V_k(\mathbf{x}) = J_2 \|\mathbf{x}\|^2 + J_4 \|\mathbf{x}\|^4_4 + \mathbf{b}_k \cdot \mathbf{x} + \tfrac{1}{2}\mathbf{x}^\top J \mathbf{x}.
\end{equation}
Computing gradients of (\ref{eq:contrastive}):

\emph{For shared couplings $J_{ij}$}: Since $\partial V_k / \partial J_{ij} = x_i x_j$ is independent of $k$, we have
\begin{equation}
\frac{\partial L_{\rm contrastive}}{\partial J_{ij}} = \sum_k \left[ x_{k,i} x_{k,j} - \frac{K-1}{K-1} x_{k,i} x_{k,j} \right] = 0.
\end{equation}
The contrastive loss \emph{does not affect} the shared couplings---the positive and negative terms cancel exactly.

\emph{For class biases $\mathbf{b}_k$}: Since $\partial V_k / \partial b_{k,i} = x_i$ and $b_k$ only appears in $V_k$:
\begin{align}
\frac{\partial L_{\rm contrastive}}{\partial b_{k,i}} &= x_{k,i}, \label{eq:hebb_pos}\\
\frac{\partial L_{\rm contrastive}}{\partial b_{j,i}} &= -\frac{x_{k,i}}{K-1} \quad (j \neq k). \label{eq:hebb_neg}
\end{align}
This yields a \textbf{Hebbian/anti-Hebbian learning rule}:
\begin{align}
\Delta b_{k,i} &\propto -x_{k,i} \quad \text{(pull correct class toward data)}, \\
\Delta b_{j,i} &\propto +\frac{x_{k,i}}{K-1} \quad \text{(push incorrect classes away)}.
\end{align}
Geometrically: the correct-class constraint surface $\mathcal{C}_k$ is pulled toward the data point, while incorrect-class surfaces $\mathcal{C}_{j\neq k}$ are pushed away.
This is analogous to Hopfield learning with explicit negative examples, or the positive/negative phase of contrastive divergence.

\textbf{Key insight:} The shared couplings $J_{ij}$ are invariant under contrastive learning (the positive and negative terms cancel).
Only the class-specific biases $\mathbf{b}_k$ change.
This means trajectory action learning (for $J_{ij}$) and contrastive learning (for $\mathbf{b}_k$) can be combined: the former learns shared structure, the latter learns discriminative boundaries.

\textbf{Detailed balance in class space.}
While Langevin dynamics provides detailed balance in \emph{configuration space} $\mathbf{x}$:
\begin{equation}
d\mathbf{x} = -\nabla V_k \, dt + \sqrt{2k_{\rm B}T}\, d\mathbf{W}, \quad p(\mathbf{x}|k) \propto e^{-V_k(\mathbf{x})/k_{\rm B}T},
\end{equation}
the contrastive objective provides detailed balance in \emph{class space}. Define
\begin{equation}
p(k|\mathbf{x}) = \frac{\exp(-V_k(\mathbf{x})/T)}{\sum_j \exp(-V_j(\mathbf{x})/T)},
\label{eq:class_softmax}
\end{equation}
a Boltzmann distribution over discrete class labels. At low temperature $T \to 0$, this concentrates on $k^* = \arg\min_k V_k(\mathbf{x})$. The contrastive loss ensures this ``classification dynamics'' has the correct fixed point: data from class $k$ should flow to class $k$.

\textbf{Joint distribution and RBM connection.}
The class-conditional structure reveals a connection to Restricted Boltzmann Machines. Consider the joint distribution
\begin{equation}
p(\mathbf{x}, k) \propto \exp\left(-V_k(\mathbf{x})/k_{\rm B}T\right).
\label{eq:joint}
\end{equation}
This treats $k$ as a \emph{one-hot hidden unit}---analogous to an RBM where the hidden layer has exactly one active unit selecting among $K$ energy functions. The marginal over $\mathbf{x}$ is
\begin{equation}
p(\mathbf{x}) = \sum_k p(\mathbf{x},k) \propto \sum_k \exp(-V_k(\mathbf{x})/k_{\rm B}T),
\end{equation}
creating an implicit mixture model without requiring explicit hidden-unit sampling during training.

\textbf{Alternating dynamics.}
The joint distribution (\ref{eq:joint}) suggests an alternating inference procedure satisfying detailed balance:
\begin{enumerate}
\item \emph{Langevin step in $\mathbf{x}$}: Given class $k$, evolve $\mathbf{x}$ under $-\nabla V_k$.
\item \emph{Gibbs step in $k$}: Given $\mathbf{x}$, resample $k \sim p(k|\mathbf{x})$ via (\ref{eq:class_softmax}).
\end{enumerate}
This provides a principled way to jointly sample configurations and class labels, unifying generation and classification in a single thermodynamic framework.

Figure~\ref{fig:biases} shows the learned class biases $-\mathbf{b}_k$, which function as attractor templates analogous to Hopfield stored patterns.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{class_biases.png}
\caption{Learned class biases $-\mathbf{b}_k$ for digits 0--9. Red indicates pixels that the class ``wants'' to be positive (white), blue indicates pixels pulled toward negative (black). The biases resemble smoothed digit templates.}
\label{fig:biases}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{contrastive_biases.png}
\caption{Contrastive signal: $-b_k + \frac{1}{K-1}\sum_{j\neq k} b_j$ for each class. This shows what makes each digit \emph{unique}---the discriminative features that separate it from other classes. Note how digit 1 emphasizes the vertical stroke while suppressing surrounding regions.}
\label{fig:contrastive}
\end{figure}

For now, the system excels at \emph{conditional} tasks (generation and denoising given the class label) rather than pure \emph{recognition}; contrastive fine-tuning to improve classification accuracy remains future work.

\subsection{Hardware Interpretation}
The class-conditional structure maps directly to hardware: the reconfiguration fabric stores $K$ bias banks $\{\mathbf{b}_0, \ldots, \mathbf{b}_{K-1}\}$.
Recognition requires evaluating $K$ energies, which can be done in parallel using $K$ copies of the gradient computation circuit, followed by a winner-take-all selection.
Once $k^*$ is determined, the appropriate bank is loaded ($\sim$ns DAC settling), and the T-array relaxes to produce the output.
This provides a physical implementation of content-addressable memory with learned attractors.

\subsection{Quantum-Classical Correspondence}
The Onsager-Machlup action developed in Section~\ref{sec:constraint} reveals a deeper structure.
The Langevin dynamics in $d$ dimensions corresponds to a $(d{+}1)$-dimensional classical field theory, where time $t$ plays the role of an extra spatial dimension.
The trajectory probability $P[\mathbf{x}(t)] \propto \exp(-S[\mathbf{x}])$ with action (\ref{eq:OM_action}) defines this higher-dimensional system.

This correspondence illuminates why learning at criticality is efficient: operating p-bits at threshold corresponds to the critical point of the $(d{+}1)$-dimensional theory, where temporal correlations become long-range.
The ``strong coupling in time'' allows information to propagate efficiently across the time dimension, meaning that each trajectory strongly constrains model parameters.
This is the physical origin of the rapid convergence observed in Figure~\ref{fig:convergence}---criticality maximizes the information content per trajectory, and the constraint surface becomes maximally informative.

\section{Derivations}
Detailed derivations of gradient estimators, time-reversal likelihoods, and relaxation bounds will be provided in an expanded version.

\end{document}
